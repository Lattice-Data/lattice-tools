{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import os\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#update to local directory to hold files\n",
    "download_dir = '/Users/jason/Downloads/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portal_obs_fields = [\n",
    "    'assay',\n",
    "    'cell_type',\n",
    "    'development_stage',\n",
    "    'disease',\n",
    "    'ethnicity',\n",
    "    'organism',\n",
    "    'sex',\n",
    "    'tissue'\n",
    "]\n",
    "full_obs_standards = portal_obs_fields + [e + '_ontology_term_id' for e in portal_obs_fields] + ['is_primary_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in the file name\n",
    "file = 'my_matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish the AnnData object\n",
    "adata = sc.read_h5ad(download_dir + file + '.h5ad') #add backed='r' if only looking at metadata\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needed, transfer to sparse matrix format\n",
    "if type(adata.X) != sparse.csr.csr_matrix:\n",
    "    print('converting X to sparse')\n",
    "    adata.X = sparse.csr_matrix(adata.X)\n",
    "if adata.raw:\n",
    "    if type(adata.raw.X) != sparse.csr.csr_matrix:\n",
    "        print('converting raw.X to sparse')\n",
    "        raw_adata = ad.AnnData(adata.raw.X, var=adata.raw.var, obs=adata.obs)\n",
    "        raw_adata.X = sparse.csr_matrix(raw_adata.X)\n",
    "        adata.raw = raw_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK MAX OF EACH LAYER\n",
    "#should not be equal, raw should be whole, positive, 10^3\n",
    "if adata.raw:\n",
    "    print('raw min = ' + str(adata.raw.X.min()))\n",
    "    print('raw max = ' + str(adata.raw.X.max()))\n",
    "print('X min = ' + str(adata.X.min()))\n",
    "print('X max = ' + str(adata.X.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if they are redundant, delete raw\n",
    "del adata.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for additional layers - they may or may not be valuable\n",
    "#conversions from Seurat often place the raw counts in adata.layers['counts']\n",
    "adata.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if additional layers, check min/max to look for redundancy\n",
    "print('X min = ' + str(adata.layers['counts'].min()))\n",
    "print('X max = ' + str(adata.layers['counts'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm at least one set of embeddings present\n",
    "adata.obsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for default_embedding value in obsm_keys()\n",
    "if 'default_embedding' in adata.uns:\n",
    "    de = adata.uns['default_embedding']\n",
    "    if de not in adata.obsm_keys():\n",
    "        print('ERROR:' + de + ' not in ' + adata.obsm_keys())\n",
    "    else:\n",
    "        print(de + ' is in ' + ','.join(adata.obsm_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for uns schema fields\n",
    "uns_schema =['schema_version','title','X_normalization']\n",
    "for p in uns_schema:\n",
    "    print(p + ': ' + adata.uns.get(p,'MISSING'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browse all of uns\n",
    "adata.uns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the portal fields are not used\n",
    "#ensure values for obs schema fields are valid\n",
    "for o in full_obs_standards:\n",
    "    print(o)\n",
    "    if o not in adata.obs_keys():\n",
    "        print('NOT IN OBS')\n",
    "    else:\n",
    "        un = adata.obs[o].unique()\n",
    "        if un.dtype == 'category':\n",
    "            print(un.to_list())\n",
    "        else:\n",
    "            print(un.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change a portal-reserved name, if needed\n",
    "adata.obs.rename(columns={'cell_type': 'author_cell_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for _colors fields & ensure they match obs fields\n",
    "numb_types = ['int_', 'int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64','float_', 'float16', 'float32', 'float64']\n",
    "\n",
    "for k in adata.uns.keys():\n",
    "    if k.endswith('_colors'):\n",
    "        obs_field = k[:-(len('_colors'))]\n",
    "        if obs_field not in adata.obs.keys():\n",
    "            print('DELETE uns.' + k)\n",
    "        else:\n",
    "            if len(adata.uns[k]) != len(adata.obs[obs_field].unique()):\n",
    "                print(f'uns.{k} is {str(len(adata.uns[k]))} but obs.{obs_field} is {str(len(adata.obs[obs_field].unique()))}')\n",
    "            if adata.obs.dtypes[obs_field].name in numb_types:\n",
    "                print(f'{obs_field} is non-categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check 1000 random barcodes against 10x lists\n",
    "def TENx_barcode_checker(df):\n",
    "    v2_file = 'cellranger-whitelist/737K-august-2016.txt'\n",
    "    v3_file = '3M-february-2018.txt'\n",
    "    v2_barcode_list = [line.strip() for line in open(v2_file, 'r')]\n",
    "    v3_barcode_list = [line.strip() for line in open(v3_file, 'r')]\n",
    "\n",
    "    cellcount = df.index.shape[0]\n",
    "    barcode_pattern = '[ACTG]{16}'\n",
    "    barcode_results = ''\n",
    "    if re.search(barcode_pattern, df.index[5,]):\n",
    "        cellcount\n",
    "        random_indices = [randint(0, cellcount - 1) for p in range(0, 1000)]\n",
    "        barcodes = {'v2': 0,'v3': 0,'both': 0,'neither': 0}\n",
    "        for i in random_indices:\n",
    "            if re.search(barcode_pattern, df.index[i,]):\n",
    "                barcode = re.search(barcode_pattern, df.index[i,]).group(0)\n",
    "                if barcode in v2_barcode_list and barcode in v3_barcode_list:\n",
    "                    barcodes['both'] += 1\n",
    "                elif barcode in v2_barcode_list:\n",
    "                    barcodes['v2'] += 1\n",
    "                elif barcode in v3_barcode_list:\n",
    "                    barcodes['v3'] += 1\n",
    "                else:\n",
    "                    barcodes['neither'] += 1\n",
    "        print(json.dumps(barcodes, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the barcode checker to clarify 3' v2 or v3\n",
    "#note: the 10x 5' v1 and v2 use the same barcode list as 3' v2\n",
    "import json\n",
    "import re\n",
    "from random import randint\n",
    "\n",
    "\n",
    "for a in adata.obs['assay_ontology_term_id'].value_counts().keys():\n",
    "    print(a)\n",
    "    TENx_barcode_checker(adata.obs[adata.obs['assay_ontology_term_id'] == a])\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for general obs field issues and collect obs information to check for redundant information\n",
    "long_fields = []\n",
    "gradient_fields = []\n",
    "uber_dict = {}\n",
    "for o in adata.obs.keys():\n",
    "    vc_dict = adata.obs[o].value_counts(dropna=False).to_dict()\n",
    "    counts = '_'.join([str(c) for c in vc_dict.values()])\n",
    "    count_len = len(vc_dict.keys())\n",
    "    values = [str(i) for i in vc_dict.keys()]\n",
    "\n",
    "    if o.startswith(' ') or o.endswith(' ') or '  ' in o:\n",
    "        print('leading/trailing whitespace:' + o)\n",
    "\n",
    "    if o not in full_obs_standards and ' '.join(o.split()).lower() in full_obs_standards:\n",
    "        print('schema conflict:' + o)\n",
    "\n",
    "    if count_len == 1:\n",
    "        lone_v = str(list(vc_dict.keys())[0])\n",
    "        if o not in full_obs_standards:\n",
    "            print('all same value:' + o + ',' + lone_v)\n",
    "\n",
    "    numb_types = ['int_', 'int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64','float_', 'float16', 'float32', 'float64']\n",
    "    if adata.obs.dtypes[o].name in numb_types:\n",
    "        gradient_fields.append(o)\n",
    "    #check for long categories as they will not be enabled for coloring\n",
    "    elif count_len > 200:\n",
    "        long_fields.append(o)\n",
    "\n",
    "    #report value_counts to later look for redundancy\n",
    "    metadata = {\n",
    "        'values': values,\n",
    "        'property': o\n",
    "    }\n",
    "    if counts in uber_dict:\n",
    "        uber_dict[counts].append(metadata)\n",
    "    else:\n",
    "        uber_dict[counts] = [metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comb value_counts to report possible redundancy\n",
    "for k,v in uber_dict.items():\n",
    "    if '_' in k and not k.startswith('1_1'):\n",
    "        props = [e['property'] for e in v]\n",
    "        if len(v) > 1 and not all(elem in full_obs_standards for elem in props):\n",
    "            print('cells breakdown: ' + k)\n",
    "            for e in v:\n",
    "                print(e['property'])\n",
    "                #print(e['values'])\n",
    "            print('----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate any fields that may be redundant\n",
    "adata.obs[['author_cell_type','cell_type_category','cell_type_ontology_term_id']].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for fields that aren't appropriate as gradient (cluster number)\n",
    "gradient_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update gradient to categorical, if needed\n",
    "adata.obs['cluster'] = adata.obs['cluster'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list any categorical fields with more than 200 categories as they may not be useful in the visualization\n",
    "long_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list any obs columns to remove\n",
    "obs_remove = [\n",
    "    'tissue',\n",
    "    'organism',\n",
    "    'ethnicity',\n",
    "    'assay',\n",
    "    'disease',\n",
    "    'sex',\n",
    "    'cell_type'\n",
    "]\n",
    "\n",
    "#remove the columns\n",
    "obs_remove = [o for o in obs_remove if o in adata.obs.columns]\n",
    "adata.obs.drop(columns=obs_remove, inplace=True)\n",
    "'removed: ' + ','.join(obs_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review obs\n",
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for ensembl IDs and redundant var fields\n",
    "#check for feature_biotype, feature_is_filtered\n",
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar review for raw.var, if present\n",
    "adata.raw.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if CellRanger count was used, check against the default references for matches in order to inform symbol-to-ID mapping\n",
    "CR_12 = 'refdata-cellranger-GRCh38-1_2_0_genes_gtf.tsv'\n",
    "CR_30 = 'refdata-cellranger-GRCh38-3_0_0_genes_gtf.tsv'\n",
    "CR_2020 = 'refdata-gex-GRCh38-2020-A_genes_gtf.tsv'\n",
    "CR_hg19 = 'refdata-cellranger-hg19-1_2_0_genes_gtf.tsv'\n",
    "for v in [CR_12,CR_30,CR_2020,CR_hg19]:\n",
    "    map_df = pd.read_csv(v, sep='\\t')\n",
    "    print(v)\n",
    "    print(adata.var.merge(map_df,left_index=True,right_on='gene_symbols',how='inner').shape[0])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in the mapping file to use to map symbols to Ensembl IDs\n",
    "#expecting a .tsv with gene_symbols column + gene_ids column\n",
    "var_mapping_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view what features are not mapped in this\n",
    "var_map_df = pd.read_csv(var_mapping_file, sep='\\t')\n",
    "adata.var[adata.var.index.isin(var_map_df['gene_symbols']) != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the list of approved IDs to filter on\n",
    "#download files from https://github.com/chanzuckerberg/single-cell-curation/tree/main/cellxgene_schema_cli/cellxgene_schema/ontology_files\n",
    "ref_files = [\n",
    "    'genes_ercc.csv',\n",
    "    'genes_homo_sapiens.csv',\n",
    "    'genes_mus_musculus.csv',\n",
    "    'genes_sars_cov_2.csv'\n",
    "]\n",
    "\n",
    "if not os.path.exists('approved_ids.csv'):\n",
    "    ids = pd.DataFrame()\n",
    "    for f in ref_files:\n",
    "        df = pd.read_csv(f, names=['feature_id','symb','num'],dtype='str')\n",
    "        ids = ids.append(df)\n",
    "        os.remove(f)\n",
    "    ids.to_csv('approved_ids.csv',index=False)\n",
    "\n",
    "approved = pd.read_csv('approved_ids.csv',dtype='str')['feature_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in any var fields to remove (typically symbols as the portal will add those)\n",
    "var_remove = [\n",
    "    'gene_symbols'\n",
    "]\n",
    "\n",
    "#typically, these fields are lacking and the feature_is_filtered should be false, but confirm that is true\n",
    "adata.var['feature_biotype'] = 'gene'\n",
    "adata.var['feature_is_filtered'] = False\n",
    "\n",
    "#map the Ensembl IDs\n",
    "adata.var = adata.var.merge(var_map_df,left_index=True,right_on='gene_symbols',how='left').set_index(adata.var.index)\n",
    "\n",
    "#filter out genes that don't appear in the approved annotation\n",
    "var_to_keep = adata.var.index.tolist()\n",
    "var_in_approved = adata.var.index[adata.var['gene_ids'].isin(approved)].tolist()\n",
    "var_to_keep = [e for e in var_to_keep if e in var_in_approved]\n",
    "adata = adata[:, var_to_keep]\n",
    "adata.var.set_index('gene_ids',inplace=True)\n",
    "\n",
    "#drop the necessary columns\n",
    "adata.var.drop(columns=var_remove, inplace=True)\n",
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat much of the same steps for the raw.var, if it exists\n",
    "adata.raw.var['feature_biotype'] = 'gene'\n",
    "\n",
    "raw_adata = ad.AnnData(adata.raw.X, var=adata.raw.var, obs=adata.obs)\n",
    "\n",
    "raw_adata.var = raw_adata.var.merge(var_map_df,left_index=True,right_on='gene_symbols',how='left').set_index(raw_adata.var.index)\n",
    "\n",
    "raw_adata = raw_adata[:, var_to_keep]\n",
    "raw_adata.var.set_index('gene_ids',inplace=True)\n",
    "raw_adata.var.drop(columns=var_remove, inplace=True)\n",
    "adata.raw = raw_adata\n",
    "adata.raw.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the cells to ensure they cluster by cell type\n",
    "default_embedding = adata.uns.get('default_embedding',adata.obsm_keys()[0])\n",
    "\n",
    "sc.set_figure_params(dpi=150)\n",
    "sc.pl.embedding(adata, basis=default_embedding, color=['cell_type_ontology_term_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the file \n",
    "new_one = file + '_revised.h5ad'\n",
    "adata.write(filename=download_dir + new_one, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
