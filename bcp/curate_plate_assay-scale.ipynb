{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd18dbd-2754-4d6e-bb48-188296a8b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import tarfile\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import sys\n",
    "import fsspec\n",
    "\n",
    "from io import BytesIO\n",
    "from dataclasses import dataclass\n",
    "from urllib.request import Request,urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from cellxgene_schema.write_labels import AnnDataLabelAppender\n",
    "from urllib.parse import quote\n",
    "from cellxgene_ontology_guide.supported_versions import CXGSchema, load_supported_versions\n",
    "from cellxgene_ontology_guide.ontology_parser import OntologyParser\n",
    "ontology_parser = OntologyParser()\n",
    "fs = fsspec.filesystem('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "136d936b-17b1-4a35-88e4-4a8cca8d24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 's3://czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/'\n",
    "uri = URIPath(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be51932c-8742-4487-a354-039cf8026669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = ['R109-25-0361.QSR-8_anndata.h5ad','R109-25-0422.QSR-1_anndata.h5ad','R109-25-0361.QSR-1_anndata.h5ad']\n",
    "pattern = r\"\\.([^_]+)_\"\n",
    "\n",
    "# List comprehension (keeps duplicates: ['QSR-8', 'QSR-1', 'QSR-1'])\n",
    "results_set = {re.search(pattern, f).group(1) for f in files if re.search(pattern, f)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98e1aaef-010b-464c-b822-3e0456e5510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = 's3://czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/'\n",
    "uri_obj = URIPath(full_uri=uri)\n",
    "\n",
    "sample_metadata = LatticeMetadata(sheet_id='', tab_name='sample template')\n",
    "sample_metadata.metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49b36960-c0ff-4f6f-9cdf-eee899d38617",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_single(uri_obj):\n",
    "    experiment_04 = SingleSublibrary(metadata=sample_metadata, uri=uri_obj)\n",
    "else:\n",
    "    experiment_04 = MultiSublibrary(metadata=sample_metadata, uri=uri_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb1689b4-021a-4ba6-8f8f-c024e23523f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples/R096-25-0255.QSR-8.allCells.csv',\n",
       " 'czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples/R096-25-0255.QSR-8_anndata.h5ad',\n",
       " 'czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples/R096-25-0256.QSR-8.allCells.csv',\n",
       " 'czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples/R096-25-0256.QSR-8_anndata.h5ad',\n",
       " 'czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples/R096-25-0257.QSR-8.allCells.csv',\n",
       " 'czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples/R096-25-0257.QSR-8_anndata.h5ad',\n",
       " 'czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples/R096-25-0258.QSR-8.allCells.csv',\n",
       " 'czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples/R096-25-0258.QSR-8_anndata.h5ad',\n",
       " 'czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples.csv']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_04.files_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "234f4b2c-2180-48c0-989a-9ee6c3d97618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/samples.csv to temp_plate/samples.csv\n"
     ]
    }
   ],
   "source": [
    "experiment_04.download_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "99da15d5-394f-4393-ac3d-b1c9d1fcc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWN_DIR = 'temp_plate'\n",
    "\n",
    "@dataclass\n",
    "class URIPath:\n",
    "    '''\n",
    "    Dataclass to hold various aspects of a single run of SeqSuite.\n",
    "    This path should include the entire URI path, including bucket name,\n",
    "    of the SeqSuite rundate output files. Ex: s3://czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/\n",
    "    '''\n",
    "    \n",
    "    full_uri: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.full_uri.endswith('/'):\n",
    "            self.full_uri += '/'\n",
    "        self.bucket_name: str = self.full_uri.split('/')[2]\n",
    "        self.prefix: str = '/'.join(self.full_uri.split('/')[3:])\n",
    "        self.sample_files = fs.ls(f's3://{self.bucket_name}/{self.prefix}samples/')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LatticeMetadata:\n",
    "    '''\n",
    "    Dataclass to hold and map ontologies to a Lattice metadata spreadsheet.\n",
    "    The functions allow for this object to read in a specific tab for a google sheet into a dataframe\n",
    "    '''\n",
    "    \n",
    "    sheet_id: str\n",
    "    tab_name: str\n",
    "\n",
    "    def get_gid(self):\n",
    "        '''\n",
    "        Given sheet id and tab name, return gid\n",
    "        '''\n",
    "        sheet_url = f'https://docs.google.com/spreadsheets/d/{self.sheet_id}'\n",
    "        req = Request(sheet_url, headers={'User-Agent' : \"Magic Browser\"})\n",
    "        s = urlopen(req)\n",
    "        soup = BeautifulSoup(s, 'html.parser')\n",
    "        tab_ids = {}\n",
    "        pattern = re.compile('var bootstrapData = (.*?)};')\n",
    "        for s in soup.find_all('script'):\n",
    "            if pattern.search(str(s)):\n",
    "                d = pattern.search(str(s)).group()[20:-1]\n",
    "                data = json.loads(d)\n",
    "                for t in data['changes']['topsnapshot']:\n",
    "                    u = t[1].split('\"')\n",
    "                    if len(u) > 5:\n",
    "                        tab_ids[u[5]] = u[1]\n",
    "        return tab_ids[self.tab_name]\n",
    "\n",
    "    def get_metadata_df(self):\n",
    "        '''\n",
    "        Given sheet id and gid, return lattice metadata in a dataframe\n",
    "        '''\n",
    "        url = f'https://docs.google.com/spreadsheets/d/{self.sheet_id}/export?format=csv&gid={self.gid}'\n",
    "        response = requests.get(url)\n",
    "        sample_df = pd.read_csv(BytesIO(response.content), comment=\"#\", dtype=str).dropna(axis=1,how='all')\n",
    "        return sample_df\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.gid = self.get_gid()\n",
    "        self.metadata_df = self.get_metadata_df()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    '''\n",
    "    Dataclass to hold various aspects of a single experiment, including\n",
    "    the pseudosamples and corresponding sample names found in SeqSuite run\n",
    "    '''\n",
    "    metadata: LatticeMetadata\n",
    "    uri: URIPath\n",
    "\n",
    "    def map_sample_ids():\n",
    "        '''\n",
    "        Diven a lattice metadata and a samples.csv file, map the pseudosample and sample ids using 'RT_index' and 'barcodes' columns, respectively\n",
    "        '''\n",
    "        return sample_ids\n",
    "\n",
    "    def download_files(self):\n",
    "        '''\n",
    "        Download all files needed for this experiment\n",
    "        '''\n",
    "        os.makedirs(DOWN_DIR, exist_ok=True)\n",
    "        for s3_uri in self.files_to_download:\n",
    "            filename = s3_uri.split('/')[-1]\n",
    "            local_path = os.path.join(DOWN_DIR, filename)\n",
    "            if not os.path.exists(local_path):\n",
    "                print(f'Downloading from {s3_uri} to {local_path}')\n",
    "                fs.get(s3_uri, local_path)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.sample_ids: list = map_sample_ids()\n",
    "\n",
    "\n",
    "class SingleSublibrary(Experiment):\n",
    "    '''\n",
    "    Plate Experiment to store specific variables and methods for an experiment that is a single sublibrary,\n",
    "    which will include *allCell.csv files in addition to the sublibrary specific h5ad\n",
    "    '''\n",
    "    \n",
    "    def determine_files(self):\n",
    "        '''\n",
    "        Return the actual file URIs of files that need to be downloaded\n",
    "        '''\n",
    "        want_files = []\n",
    "        for file in self.uri.sample_files:\n",
    "            if file.endswith(('allCells.csv','_anndata.h5ad')):\n",
    "               want_files.append(file) \n",
    "        want_files.append(f'{self.uri.bucket_name}/{self.uri.prefix}samples.csv')\n",
    "        return want_files\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.files_to_download: list = self.determine_files()\n",
    "\n",
    "\n",
    "class MultiSublibrary(Experiment):\n",
    "    '''\n",
    "    Plate Experiment to store specific variables and methods for an experiment that has multiple sublibraries,\n",
    "    which will include the sample merged h5ad files\n",
    "    '''\n",
    "    def determine_files():\n",
    "        '''\n",
    "        Return the actual file URIs of files that need to be downloaded\n",
    "        '''\n",
    "        want_files = []\n",
    "        for file in self.uri.sample_files:\n",
    "            if file.endswith('anndata.h5ad') and not re.search('QSR', file):\n",
    "                want_files.append(file)\n",
    "        want_files.append(f'{self.uri.bucket_name}/{self.uri.prefix}samples.csv')\n",
    "        return want_files\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.files_to_download: list = self.determine_files()       \n",
    "\n",
    "\n",
    "def is_single(uripath_obj):\n",
    "    '''\n",
    "    Given an URI object, determine if the experiment is a single sublibrary experiment\n",
    "    by searching for 'QSR-#' for files on S3\n",
    "    '''\n",
    "    files = uripath_obj.sample_files\n",
    "    pattern = r'\\.(QSR-[1-8])_anndata.h5ad'\n",
    "    \n",
    "    sublibraries = list({re.search(pattern, f).group(1) for f in files if re.search(pattern, f)})\n",
    "    return len(sublibraries) == 1\n",
    "\n",
    "\n",
    "# Define Scale \"sample\" to Lattice metadata \"pseudosample\"\n",
    "# experiment_name is the corresponding experiment in the Lattice spreadsheet\n",
    "sample_map = {\n",
    "#     'R096-25-0255.QSR-8':'bl1',\n",
    "#     'R096-25-0256.QSR-8':'bl2',\n",
    "#     'R096-25-0257.QSR-8':'bl3',\n",
    "#     'R096-25-0258.QSR-8':'bl4'\n",
    "# # }\n",
    "    'R109-25-0422':'25.0422',\n",
    "    'R109-25-0423':'25.0423',\n",
    "    'R109-25-0424':'25.0424',\n",
    "    'R109-25-0425':'25.0425',\n",
    "    'R109-25-0426':'25.0426',\n",
    "    'R109-25-0427':'25.0427',\n",
    "    'R109-25-0428':'25.0428',\n",
    "    'R109-25-0429':'25.0429',\n",
    "    'R109-25-0430':'25.0430',\n",
    "    'R109-25-0431':'25.0431',\n",
    "    'R109-25-0432':'25.0432',\n",
    "    'R109-25-0433':'25.0433',\n",
    "    'R109-25-0434':'25.0434'\n",
    "}\n",
    "    \n",
    "set_rundate = 'Run_2025-12-15_biohub'\n",
    "pseudosamples = list(sample_map.keys())\n",
    "experiment_name = 'CHEM4_R109'\n",
    "down_dir = 'temp_plate'\n",
    "\n",
    "    \n",
    "\n",
    "def download_files(s3client, bucket, prefix):\n",
    "    '''\n",
    "    given s3 directory, bucket and prefix (proj/order/experiment_name), download the following files, one set for each pseudosample:\n",
    "        - *_anndata.h5ad\n",
    "        - *.merged.allCells.csv\n",
    "    '''\n",
    "    files = ['_anndata.h5ad',\n",
    "             '.merged.allCells.csv']\n",
    "    \n",
    "    if os.path.exists(down_dir) == False:\n",
    "        os.mkdir(down_dir)\n",
    "\n",
    "    for pseudosample in pseudosamples:\n",
    "        print(f'Downloaoding for {pseudosample}')\n",
    "        rundates = get_subdirectories(s3client, bucket, prefix+'/processed/')\n",
    "        if len(rundates)>1:\n",
    "            print(f'WARNING: more than one rundate: {rundate}')\n",
    "            rundate = set_rundates\n",
    "        else:\n",
    "            rundate = rundates[0].split('/')[-2]\n",
    "        for file in files:\n",
    "            file_path = f'{prefix}/processed/{rundate}/samples/{pseudosample}{file}'\n",
    "            down_path = f'{down_dir}/{pseudosample}{file}'\n",
    "            if not os.path.exists(down_path):\n",
    "                s3client.download_file(bucket, file_path, down_path)\n",
    "\n",
    "\n",
    "def assign_oligo(adata, hash_csv):\n",
    "    '''\n",
    "    given an allCell.csv, with \"cell_id\" colum for cellID and \"assigned_scaleplex\" for scaleplex hash assignment,\n",
    "    merge \"assigned_scaleplex\" to adata.obs, where the well number and letter need to be swapped (eg: 7E -> E7) and add prefix \"SCALE-\"\n",
    "    to match the Lattice metadata spreadsheet. Drop cells that have the following values as \"assigned_scaleplex\":\n",
    "     - Indeterminate\n",
    "     - Max_Fail\n",
    "     - Unexpected\n",
    "    '''\n",
    "    drop_values = ['Indeterminate','Max_Fail','Unexpected']\n",
    "    hash_df = pd.read_csv(hash_csv, index_col = 'cell_id')\n",
    "    adata_subset = adata.copy()\n",
    "    #adata_subset.obs = pd.merge(adata_subset.obs, hash_df[['passing_scaleplex', 'assigned_scaleplex']], left_index=True, right_index=True, how='left')\n",
    "    #adata_subset.obs['assigned_scaleplex'] = adata_subset.obs['assigned_scaleplex'].astype('string')\n",
    "    adata_subset = adata_subset[~adata_subset.obs['assigned_scaleplex'].isin(drop_values)].copy()\n",
    "    adata_subset.obs['assigned_hash_index'] = adata_subset.obs['assigned_scaleplex'].str.replace(r'(\\d+)([a-zA-Z])', r'SCALE-\\2\\1', regex=True).astype('string')\n",
    "    return adata_subset\n",
    "\n",
    "\n",
    "def cxg_add_labels(adata):\n",
    "    adata.obs['cell_type_ontology_term_id'] = 'unknown'\n",
    "    labeler = AnnDataLabelAppender(adata)\n",
    "    labeler._add_labels()\n",
    "    adata.obs.drop(columns=['cell_type_ontology_term_id','cell_type'],inplace=True)\n",
    "\n",
    "    schema_v = labeler.schema_version\n",
    "    adata.uns['schema_version'] = schema_v\n",
    "    adata.uns['schema_reference'] = labeler._build_schema_reference_url(schema_v)\n",
    "\n",
    "\n",
    "def add_guide_metadata(adata, sheet, guide_gid):\n",
    "    '''\n",
    "    Add guide metadata into adata.uns from Lattice wrangling sheet\n",
    "    \n",
    "    :param obj adata: the anndata object that is being transformed into the curated matrix\n",
    "    :param obj guide_df: the dataframe containing guide metadata from wrangling sheet\n",
    "    \n",
    "    :returns obj adata: modified adata to contain guide metadata\n",
    "    '''\n",
    "    url = f'https://docs.google.com/spreadsheets/d/{sheet}/export?format=csv&gid={guide_gid}'\n",
    "    response = requests.get(url)\n",
    "    guide_df = pd.read_csv(BytesIO(response.content), comment=\"#\", dtype=str)\n",
    "    genetic_perturbations = {}\n",
    "    \n",
    "    for row in guide_df.itertuples():\n",
    "        genetic_perturbations[row.guide_id] = {}\n",
    "        genetic_perturbations[row.guide_id]['role'] = 'targeting' if row.guide_role == 'Targeting a Gene' else 'control'\n",
    "        genetic_perturbations[row.guide_id]['protospacer_sequence'] = row.guide_protospacer\n",
    "        genetic_perturbations[row.guide_id]['protospacer_adjacent_motif'] = row.guide_PAM\n",
    "        if not pd.isna([row.start,row.end,row.strand]).all():\n",
    "            chr_loc = str(row.chromosome).replace(\"chr\",\"\") + \":\" + str(row.start) + \"-\" + str(row.end) + \"(\" + str(row.strand) + \")\"\n",
    "            genetic_perturbations[row.guide_id]['target_genomic_regions'] = [chr_loc]\n",
    "        if not pd.isna(row.overlapping_gene_ids):\n",
    "            genetic_perturbations[row.guide_id]['target_features'] = {}\n",
    "            for i in range(len(row.overlapping_gene_ids.split(\";\"))):\n",
    "                genetic_perturbations[row.guide_id]['target_features'][row.overlapping_gene_ids.split(\";\")[i]] = row.overlapping_gene_names.split(\";\")[i]\n",
    "                                                                             \n",
    "            \n",
    "    adata.uns['genetic_perturbations'] = genetic_perturbations\n",
    "    \n",
    "    return adata\n",
    "\n",
    "\n",
    "def determine_perturbation_strategy(adata):\n",
    "    '''\n",
    "    Assess feature_call from protospacer_calls_per_cell.csv, where if all guides\n",
    "    assigned to a single cell are all control, then 'control'. Otherwise, it is \"no perturbations\"\n",
    "    if no guids or one of the following if targeting:\n",
    "        - \"CRISPR activation screen\"\n",
    "        - \"CRISPR interference screen\"\n",
    "        - \"CRISPR knockout mutant\"\n",
    "        - \"CRISPR knockout screen\"\n",
    "    \n",
    "    :param obj adata: the anndata object that is being transformed into the curated matrix\n",
    "\n",
    "    :returns obj adata: modified adata to contain perturbation_strategy as cell metadata\n",
    "    '''\n",
    "    adata.obs['genetic_perturbation_strategy_calculated'] = adata.obs['genetic_perturbation_id']\n",
    "    adata.obs['genetic_perturbation_strategy_calculated'] = adata.obs['genetic_perturbation_strategy_calculated'].apply(\n",
    "        lambda x: x.split(' || ') if pd.notna(x) else 'no perturbations'\n",
    "    )\n",
    "    adata.obs['genetic_perturbation_strategy_calculated'] = adata.obs['genetic_perturbation_strategy_calculated'].apply(\n",
    "        lambda x: [adata.uns['genetic_perturbations'][i]['role'] for i in x] if isinstance(x, list)\n",
    "            else x        \n",
    "    )\n",
    "    adata.obs['genetic_perturbation_strategy_calculated'] = adata.obs['genetic_perturbation_strategy_calculated'].apply(\n",
    "         lambda x: 'control' if isinstance(x, list) and 'targeting' not in set(x)\n",
    "            else x\n",
    "    )\n",
    "    adata.obs.loc[adata.obs['genetic_perturbation_strategy_calculated']=='control', 'genetic_perturbation_strategy'] = 'control'\n",
    "    adata.obs.loc[adata.obs['genetic_perturbation_strategy_calculated']=='no perturbations', 'genetic_perturbation_strategy'] = 'no perturbations'\n",
    "    adata.obs.drop(columns=['genetic_perturbation_strategy_calculated'], inplace=True)\n",
    "    \n",
    "    return adata\n",
    "\n",
    "\n",
    "\n",
    "def map_ontologies(sample_df):\n",
    "    '''\n",
    "    Takes the sample metadata dataframe and standardizes ontologies\n",
    "    Also checks that standard fields are only filled out for appropriate organism\n",
    "\n",
    "    :param dataframe sample_df: the sample metadata from given google sheet\n",
    "\n",
    "    :returns dataframe sample_df: sample metadata with ontologies added\n",
    "    '''\n",
    "    col_ont_map = {\n",
    "        'organism':'NCBITaxon',\n",
    "        'sex':'PATO',\n",
    "        'self_reported_ethnicity':{'NCBITaxon:9606':'HANCESTRO',\n",
    "                                   'other':'none'},\n",
    "        'disease':'MONDO',\n",
    "        'assay':'EFO',\n",
    "        'development_stage':{'NCBITaxon:6239':'WBls', # C. Elegans\n",
    "                             'NCBITaxon:7227':'FBdv', # Drosophila\n",
    "                             'NCBITaxon:10090':'MmusDv', # Mouse\n",
    "                             'NCBITaxon:7955':'ZFS', # Zebrafish\n",
    "                             'other':'HsapDv' # For all other organisms, use HsapDv\n",
    "                            },\n",
    "        'tissue':{'NCBITaxon:6239':'WBbt', # C. Elegans\n",
    "                  'NCBITaxon:7227':'FBbt', # Drosophila\n",
    "                  'NCBITaxon:7955':'ZFA', # Zebrafish\n",
    "                  'other':'UBERON' # For all other organisms, use UBERON\n",
    "                 }\n",
    "    }\n",
    "    ontology_parser = OntologyParser()\n",
    "    ont_err_lst = []\n",
    "    \n",
    "    for col in col_ont_map:\n",
    "        map_dict = {}\n",
    "        for label in sample_df[col].unique():\n",
    "            term_id = None\n",
    "            if col == 'disease' and label == 'normal': # Normal is not in MONDO ontology\n",
    "                term_id = 'PATO:0000461'\n",
    "            elif label in ['unknown','na']: # Unknown and na won't be in ontologies, pass along\n",
    "                map_dict[label] = label\n",
    "                continue\n",
    "            elif col in ['tissue','development_stage','self_reported_ethnicity']:\n",
    "                if col == 'tissue':\n",
    "                    # Find what tissue type is at label row\n",
    "                    if sample_df.loc[sample_df[col] == label, 'tissue_type'].tolist()[0] != 'tissue':\n",
    "                        map_dict[label] = label # Don't map cell type in tissue\n",
    "                        continue\n",
    "                # Find what organism term id is at label row\n",
    "                org_term_id = sample_df.loc[sample_df[col] == label, 'organism_ontology_term_id'].tolist()[0]\n",
    "                if org_term_id in col_ont_map[col]:\n",
    "                    # Get ontology of specific organism and map label\n",
    "                    species_ont = col_ont_map[col][org_term_id]\n",
    "                    term_id = ontology_parser.get_term_id_by_label(label, species_ont)\n",
    "                else:\n",
    "                    if col_ont_map[col]['other'] == 'none':\n",
    "                        map_dict[label] = label\n",
    "                        continue\n",
    "                    else:\n",
    "                        term_id = ontology_parser.get_term_id_by_label(label, col_ont_map[col]['other'])\n",
    "            else:\n",
    "                term_id = ontology_parser.get_term_id_by_label(label, col_ont_map[col])\n",
    "            if term_id == None:\n",
    "                print(f'{label}\\t{col}\\t{term_id}\\t{org_term_id}')\n",
    "                if org_term_id:\n",
    "                    if org_term_id in col_ont_map[col]:\n",
    "                        ont_err_lst.append(f\"Error: Matching '{col_ont_map[col][org_term_id]}' term id not found for label '{label}' in column '{col}'\")\n",
    "                    else:\n",
    "                        ont_err_lst.append(f\"Error: Matching '{col_ont_map[col]['other']}' term id not found for label '{label}' in column '{col}'\")\n",
    "                else:\n",
    "                    ont_err_lst.append(f\"Error: Matching '{col_ont_map[col]}' term id not found for label '{label}' in column '{col}'\")\n",
    "                map_dict[label] = label\n",
    "                continue\n",
    "            map_dict[label] = term_id\n",
    "        sample_df[col + '_ontology_term_id'] = sample_df[col].map(map_dict)\n",
    "        del sample_df[col]\n",
    "    \n",
    "    ### Print out any errors from ontologizing\n",
    "    if ont_err_lst:\n",
    "        for e in ont_err_lst:\n",
    "            print(e)\n",
    "        sys.exit()\n",
    "\n",
    "    ### Convert string to boolean for is_pilot_data and donor_living_at_sample_collection\n",
    "    ### Check that donor_living_at_sample_collection is not filled out for non-human\n",
    "    b_type = ['is_pilot_data','donor_living_at_sample_collection']\n",
    "    for c in b_type:\n",
    "        if c in sample_df.columns:\n",
    "            if c == 'donor_living_at_sample_collection':\n",
    "                for val in sample_df[c].unique():\n",
    "                    if val != 'na' and sample_df.loc[sample_df[c] == val, \n",
    "                    'organism_ontology_term_id'].tolist()[0] != 'NCBITaxon:9606':\n",
    "                        print(f\"ERROR: donor_living_at_sample_collection for non-human data should be 'na' but '{val}' is present\")\n",
    "                        sys.exit()\n",
    "            sample_df[c] == sample_df[c].replace({'FALSE':False, 'TRUE':True})\n",
    "    \n",
    "    ### Blank fields in worksheet result in NaN values in dataframe, replacing these with na?\n",
    "    ### Could also replace with unknown for certain columns using fillna options?\n",
    "    sample_df.fillna('na', inplace=True)\n",
    "    sample_df.drop(columns=[c for c in sample_df.columns if c.startswith('!')], inplace=True)\n",
    "\n",
    "    return sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e053c02-2877-48b3-baca-3a111f41aa7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in R109-25-0422\n",
      "Reading in R109-25-0423\n",
      "Reading in R109-25-0424\n",
      "Reading in R109-25-0425\n",
      "Reading in R109-25-0426\n",
      "Reading in R109-25-0427\n",
      "Reading in R109-25-0428\n",
      "Reading in R109-25-0429\n",
      "Reading in R109-25-0430\n",
      "Reading in R109-25-0431\n",
      "Reading in R109-25-0432\n",
      "Reading in R109-25-0433\n",
      "Reading in R109-25-0434\n"
     ]
    }
   ],
   "source": [
    "# Read in h5ad file, and assign scale hash oligo to each cell\n",
    "\n",
    "adatas = []\n",
    "for pseudosample in pseudosamples:\n",
    "    print(f'Reading in {pseudosample}')\n",
    "    adata = sc.read_h5ad(f'{down_dir}/{pseudosample}_anndata.h5ad')\n",
    "    adata = assign_oligo(adata, f'{down_dir}/{pseudosample}.merged.allCells.csv')\n",
    "    adatas.append(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32aac163-8222-4882-b34e-db92b3755a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curating:\tR096-25-0255.QSR-8\n",
      "Curating:\tR096-25-0256.QSR-8\n",
      "Curating:\tR096-25-0257.QSR-8\n",
      "Curating:\tR096-25-0258.QSR-8\n"
     ]
    }
   ],
   "source": [
    "# For each adata, determine which sample (scale_sample) it is for, then subset sample_df for:\n",
    "# - the corresponding sample (lab_sample)\n",
    "# - the experiment name of the SeqSuite run\n",
    "# Then do a merged based on hash_index and assigned_hash_index\n",
    "\n",
    "for adata in adatas:\n",
    "    scale_sample = adata.obs['sample'].unique()[0]\n",
    "    print(f'Curating:\\t{scale_sample}')\n",
    "    lab_sample = sample_map[scale_sample]\n",
    "    sample_df_subset = sample_df[(sample_df['pseudosample']==lab_sample) & (sample_df['experiment_name']== experiment_name)]\n",
    "    adata.obs = pd.merge(adata.obs, sample_df_subset, left_on='assigned_hash_index', right_on='hash_index', how='left').set_index(adata.obs.index)\n",
    "    adata.obs['is_primary_data'] = True\n",
    "\n",
    "    # Set uns metadata\n",
    "    adata.uns['organism_ontology_term_id'] = adata.obs['organism_ontology_term_id'].unique()[0]\n",
    "    adata.obs.drop(columns=['organism_ontology_term_id'], inplace=True)\n",
    "    prefix = f'{adata.obs[\"experiment_name\"].unique()[0]}__{adata.obs[\"sample\"].unique()[0]}'\n",
    "    adata.uns['title'] = prefix\n",
    "\n",
    "    # Write to file\n",
    "    out = 'curated_matrices'\n",
    "    if os.path.exists(out) == False:\n",
    "        os.mkdir(out)\n",
    "    adata.write(f'{out}/{prefix}.h5ad', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4ad3c1-19cb-4da3-8954-68c4a658f36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assay_ontology_term_id ['EFO:0022490', nan]\n",
      "\n",
      "\u001b[1m\u001b[31mERROR: cell_type_ontology_term_id not in obs\n",
      "\u001b[0m\n",
      "development_stage_ontology_term_id ['ZFS:0000035', nan]\n",
      "\n",
      "disease_ontology_term_id ['PATO:0000461', nan]\n",
      "\n",
      "self_reported_ethnicity_ontology_term_id ['na', nan]\n",
      "\n",
      "sex_ontology_term_id ['unknown', nan]\n",
      "\n",
      "tissue_ontology_term_id ['ZFA:0000103', nan]\n",
      "\n",
      "donor_id ['CHEM13_SCALE_E7_Bl1', 'CHEM13_SCALE_D8_Bl1', 'CHEM13_SCALE_G10_Bl1', 'CHEM13_SCALE_H1_Bl1', 'CHEM13_SCALE_A3_Bl1', 'CHEM13_SCALE_E9_Bl1', 'CHEM13_SCALE_H8_Bl1', 'CHEM13_SCALE_H3_Bl1', 'CHEM13_SCALE_B7_Bl1', 'CHEM13_SCALE_F2_Bl1', 'CHEM13_SCALE_D2_Bl1', 'CHEM13_SCALE_D3_Bl1', 'CHEM13_SCALE_C10_Bl1', 'CHEM13_SCALE_D1_Bl1', 'CHEM13_SCALE_E1_Bl1', 'CHEM13_SCALE_E8_Bl1', 'CHEM13_SCALE_A2_Bl1', 'CHEM13_SCALE_G8_Bl1', 'CHEM13_SCALE_F5_Bl1', 'CHEM13_SCALE_H10_Bl1', 'CHEM13_SCALE_B9_Bl1', 'CHEM13_SCALE_E6_Bl1', 'CHEM13_SCALE_G4_Bl1', 'CHEM13_SCALE_F3_Bl1', 'CHEM13_SCALE_C4_Bl1', 'CHEM13_SCALE_A1_Bl1', 'CHEM13_SCALE_D7_Bl1', 'CHEM13_SCALE_A4_Bl1', 'CHEM13_SCALE_B4_Bl1', 'CHEM13_SCALE_A9_Bl1', 'CHEM13_SCALE_B1_Bl1', 'CHEM13_SCALE_F7_Bl1', 'CHEM13_SCALE_D10_Bl1', 'CHEM13_SCALE_A6_Bl1', 'CHEM13_SCALE_D9_Bl1', 'CHEM13_SCALE_C9_Bl1', 'CHEM13_SCALE_B8_Bl1', 'CHEM13_SCALE_H6_Bl1', 'CHEM13_SCALE_E5_Bl1', 'CHEM13_SCALE_C3_Bl1', 'CHEM13_SCALE_H2_Bl1', 'CHEM13_SCALE_C1_Bl1', 'CHEM13_SCALE_C8_Bl1', 'CHEM13_SCALE_A5_Bl1', 'CHEM13_SCALE_H9_Bl1', 'CHEM13_SCALE_G6_Bl1', 'CHEM13_SCALE_E3_Bl1', 'CHEM13_SCALE_B2_Bl1', 'CHEM13_SCALE_C7_Bl1', 'CHEM13_SCALE_B5_Bl1', 'CHEM13_SCALE_H5_Bl1', 'CHEM13_SCALE_F4_Bl1', 'CHEM13_SCALE_H4_Bl1', 'CHEM13_SCALE_H7_Bl1', 'CHEM13_SCALE_F9_Bl1', 'CHEM13_SCALE_G2_Bl1', 'CHEM13_SCALE_G3_Bl1', 'CHEM13_SCALE_D4_Bl1', 'CHEM13_SCALE_E4_Bl1', 'CHEM13_SCALE_F1_Bl1', 'CHEM13_SCALE_D6_Bl1', 'CHEM13_SCALE_E2_Bl1', 'CHEM13_SCALE_B10_Bl1', 'CHEM13_SCALE_C6_Bl1', 'CHEM13_SCALE_G1_Bl1', 'CHEM13_SCALE_A7_Bl1', 'CHEM13_SCALE_F10_Bl1', 'CHEM13_SCALE_G5_Bl1', 'CHEM13_SCALE_A10_Bl1', 'CHEM13_SCALE_F8_Bl1', 'CHEM13_SCALE_G7_Bl1', 'CHEM13_SCALE_A8_Bl1', 'CHEM13_SCALE_D5_Bl1', 'CHEM13_SCALE_F6_Bl1', 'CHEM13_SCALE_B6_Bl1', 'CHEM13_SCALE_E10_Bl1', 'CHEM13_SCALE_C2_Bl1', 'CHEM13_SCALE_C5_Bl1', 'CHEM13_SCALE_B3_Bl1', nan, 'CHEM13_SCALE_G9_Bl1']\n",
      "\n",
      "suspension_type ['nucleus', nan]\n",
      "\n",
      "tissue_type ['tissue', nan]\n",
      "\n",
      "\u001b[1m\u001b[31mERROR: is_primary_data not in obs\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/jovyan/lattice-tools/cellxgene_resources')\n",
    "from cellxgene_mods import *\n",
    "evaluate_obs_schema(adatas[0].obs, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25701f1c-9a9f-41e5-8dc7-b8e7237a3e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
