{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd18dbd-2754-4d6e-bb48-188296a8b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import tarfile\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import sys\n",
    "import fsspec\n",
    "import anndata as ad\n",
    "\n",
    "from io import BytesIO\n",
    "from dataclasses import dataclass, field\n",
    "from urllib.request import Request,urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from cellxgene_schema.write_labels import AnnDataLabelAppender\n",
    "from urllib.parse import quote\n",
    "from cellxgene_ontology_guide.supported_versions import CXGSchema, load_supported_versions\n",
    "from cellxgene_ontology_guide.ontology_parser import OntologyParser\n",
    "ontology_parser = OntologyParser()\n",
    "fs = fsspec.filesystem('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1aaef-010b-464c-b822-3e0456e5510f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instatiate URIPath and metadata spreadsheeet\n",
    "uri = 's3://czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/'\n",
    "uri_obj = URIPath(full_uri=uri)\n",
    "sample_metadata = LatticeMetadata(sheet_id='', tab_name='sample template', uri=uri_obj)\n",
    "sample_metadata.metadata_df\n",
    "\n",
    "# Create experiment objecxt\n",
    "if is_single(uri_obj):\n",
    "    experiment_04 = SingleSublibrary(metadata=sample_metadata, uri=uri_obj)\n",
    "else:\n",
    "    experiment_04 = MultiSublibrary(metadata=sample_metadata, uri=uri_obj)\n",
    "\n",
    "# Download files, map pseudosa/mple and sample ids, and create pseudosamples for experiment\n",
    "experiment_04.download_files()\n",
    "experiment_04.get_sample_ids_map()\n",
    "experiment_04.add_pseudosamples()\n",
    "\n",
    "# Add sample level metrics and ontology labels\n",
    "for pseudos in experiment_04.all_pseudosamples:\n",
    "    add_sample_metadata(pseudos)\n",
    "    cxg_add_labels(pseudos.adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71234c5b-e7f8-4393-b818-de9daf5c2c4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write to file\n",
    "for pseudos in experiment_04.all_pseudosamples:\n",
    "    pseudos.adata.write(f'{DOWN_DIR}/{pseudos.sample_id}_curated.h5ad', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037045d7-939d-4fd3-b94c-3831434b0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/jovyan/lattice-tools/cellxgene_resources')\n",
    "from cellxgene_mods import *\n",
    "evaluate_obs_schema(experiment_04.all_pseudosamples[3].adata.obs, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da15d5-394f-4393-ac3d-b1c9d1fcc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWN_DIR = 'temp_plate'\n",
    "\n",
    "@dataclass\n",
    "class URIPath:\n",
    "    '''\n",
    "    Dataclass to hold various aspects of a single run of SeqSuite.\n",
    "    This path should include the entire URI path, including bucket name, of the SeqSuite rundate output files.\n",
    "    Ex: s3://czi-novogene/trapnell-seahub-bcp/NVUS2024101701-04/RNA3_096/processed/Run_2025-12-15_biohub/\n",
    "    sample_files includes all files associated with this URIPath that are needed\n",
    "    '''\n",
    "    \n",
    "    full_uri: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.full_uri.endswith('/'):\n",
    "            self.full_uri += '/'\n",
    "        self.bucket_name: str = self.full_uri.split('/')[2]\n",
    "        self.prefix: str = '/'.join(self.full_uri.split('/')[3:])\n",
    "        self.sample_files = fs.ls(f's3://{self.bucket_name}/{self.prefix}samples/')\n",
    "        self.sample_files.append(f'{self.bucket_name}/{self.prefix}samples.csv')\n",
    "        self.sample_files.append(f'{self.bucket_name}/{self.prefix}reports/allSamples.reportStatistics.csv')\n",
    "        self.experiment = self.full_uri.split('/')[5]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LatticeMetadata:\n",
    "    '''\n",
    "    Dataclass to hold and map ontologies to a Lattice metadata spreadsheet.\n",
    "    The functions allow for this object to read in a specific tab for a google sheet into a dataframe\n",
    "    '''\n",
    "    \n",
    "    sheet_id: str\n",
    "    tab_name: str\n",
    "    uri: URIPath\n",
    "\n",
    "    def get_gid(self):\n",
    "        '''\n",
    "        Given sheet id and tab name, return gid\n",
    "        '''\n",
    "        sheet_url = f'https://docs.google.com/spreadsheets/d/{self.sheet_id}'\n",
    "        req = Request(sheet_url, headers={'User-Agent' : \"Magic Browser\"})\n",
    "        s = urlopen(req)\n",
    "        soup = BeautifulSoup(s, 'html.parser')\n",
    "        tab_ids = {}\n",
    "        pattern = re.compile('var bootstrapData = (.*?)};')\n",
    "        for s in soup.find_all('script'):\n",
    "            if pattern.search(str(s)):\n",
    "                d = pattern.search(str(s)).group()[20:-1]\n",
    "                data = json.loads(d)\n",
    "                for t in data['changes']['topsnapshot']:\n",
    "                    u = t[1].split('\"')\n",
    "                    if len(u) > 5:\n",
    "                        tab_ids[u[5]] = u[1]\n",
    "        return tab_ids[self.tab_name]\n",
    "\n",
    "    def get_metadata_df(self):\n",
    "        '''\n",
    "        Given sheet id and gid, return lattice metadata in a dataframe, subset by experiment name\n",
    "        '''\n",
    "        url = f'https://docs.google.com/spreadsheets/d/{self.sheet_id}/export?format=csv&gid={self.gid}'\n",
    "        response = requests.get(url)\n",
    "        sample_df = pd.read_csv(BytesIO(response.content), comment=\"#\", dtype=str).dropna(axis=1,how='all')\n",
    "        sample_df = sample_df[sample_df['experiment_name'] == self.uri.experiment]\n",
    "        return sample_df\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.gid = self.get_gid()\n",
    "        self.metadata_df = self.get_metadata_df()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Pseudosample:\n",
    "    '''\n",
    "    The pooled pseudosample entity that by which there is a single h5ad file.\n",
    "    This object will track associated metadata and AnnData object\n",
    "    '''\n",
    "    local_h5ad: str\n",
    "    pseudosample_id: str\n",
    "    sample_id: str\n",
    "    allcells: None\n",
    "    metadata_sample: pd.DataFrame\n",
    "    parent: 'Experiment'\n",
    "\n",
    "    def load_h5ad(self):\n",
    "        '''\n",
    "        Load h5ad as AnnData object using local_h5ad variable, set uns variabls\n",
    "        '''\n",
    "        adata = ad.read_h5ad(self.local_h5ad)\n",
    "        adata.uns['title'] = f'{self.parent.uri.experiment}_{self.pseudosample_id}_{self.sample_id}'\n",
    "        return adata\n",
    "\n",
    "    def assign_oligo(self):\n",
    "        '''\n",
    "        For SingleSublibrary experiments, the observations need to have the hash oligo mapped using the *allCells.csv file\n",
    "        '''\n",
    "        hash_df = pd.read_csv(self.allcells, index_col = 'cell_id')\n",
    "        self.adata.obs = pd.merge(self.adata.obs, hash_df[['passing_scaleplex', 'assigned_scaleplex']], left_index=True, right_index=True, how='left')\n",
    "        self.adata.obs['assigned_scaleplex'] = self.adata.obs['assigned_scaleplex'].astype('string')\n",
    "    \n",
    "    def drop_unassigned(self):\n",
    "        '''\n",
    "        given an allCell.csv, with \"cell_id\" colum for cellID and \"assigned_scaleplex\" for scaleplex hash assignment,\n",
    "        merge \"assigned_scaleplex\" to adata.obs, where the well number and letter need to be swapped (eg: 7E -> E7) and add prefix \"SCALE-\"\n",
    "        to match the Lattice metadata spreadsheet. Drop cells that have the following values as \"assigned_scaleplex\":\n",
    "         - Indeterminate\n",
    "         - Max_Fail\n",
    "         - Unexpected\n",
    "        '''\n",
    "        drop_values = ['Indeterminate','Max_Fail','Unexpected']\n",
    "        adata_subset = self.adata.copy()\n",
    "        adata_subset = adata_subset[~adata_subset.obs['assigned_scaleplex'].isin(drop_values)].copy()\n",
    "        self.adata = adata_subset\n",
    "\n",
    "    def rename_cols(self):\n",
    "        '''\n",
    "        For the cell level metrics, append 'cell_' to field name, as there will be sample level metrics\n",
    "        '''\n",
    "        metrics_list = [\n",
    "            'counts',\n",
    "            'genes',\n",
    "            'totalReads',\n",
    "            'countedReads',\n",
    "            'mappedReads',\n",
    "            'geneReads',\n",
    "            'exonReads',\n",
    "            'intronReads',\n",
    "            'antisenseReads',\n",
    "            'mitoReads',\n",
    "            'countedMultiGeneReads',\n",
    "            'Saturation',\n",
    "            'mitoProp'\n",
    "        ]\n",
    "        for col in metrics_list:\n",
    "            self.adata.obs.rename(columns = {col: f'cell_{col}'}, inplace = True)\n",
    "    \n",
    "    def map_metadata(self):\n",
    "        '''\n",
    "        Add additional metadata to obs from spreadsheet\n",
    "        For any cells that map to a hash index that is not on the spreadsheet for that particular pseudosample, they are dropped and warning printed\n",
    "        Example variables:\n",
    "            assigned_scaleplex: 10C\n",
    "            assigned_hash_index: SCALE-C10 (converted from assigned_scaleplex)\n",
    "            hash_index: SCALE-C10\n",
    "        '''\n",
    "        self.adata.obs['assigned_hash_index'] = self.adata.obs['assigned_scaleplex'].str.replace(r'(\\d+)([a-zA-Z])', r'SCALE-\\2\\1', regex=True).astype('string')\n",
    "        not_found = [i for i in self.adata.obs['assigned_hash_index'] if i not in self.metadata_sample['hash_index'].to_list()]\n",
    "        if len(not_found) > 1:\n",
    "            print(f'ERROR: no hash_index found for {self.pseudosample_id} {self.sample_id}: {not_found}')\n",
    "        adata_subset = self.adata.copy()\n",
    "        adata_subset = adata_subset[~adata_subset.obs['assigned_hash_index'].isin(not_found)]\n",
    "        self.adata = adata_subset\n",
    "        self.adata.obs = pd.merge(self.adata.obs, self.metadata_sample, left_on='assigned_hash_index', right_on='hash_index', how='left').set_index(self.adata.obs.index)\n",
    "        self.adata.obs.drop(columns=['assigned_hash_index'], inplace=True)\n",
    "    \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.adata = self.load_h5ad()\n",
    "        if isinstance(self.parent, SingleSublibrary):\n",
    "            self.assign_oligo()\n",
    "        self.drop_unassigned()\n",
    "        self.rename_cols()\n",
    "        self.map_metadata()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    '''\n",
    "    Dataclass to hold various aspects of a single experiment, including\n",
    "    the pseudosamples and corresponding sample names found in SeqSuite run\n",
    "    '''\n",
    "    metadata: LatticeMetadata\n",
    "    uri: URIPath\n",
    "    all_pseudosamples: list[Pseudosample] = field(init=False, default_factory=list)\n",
    "    sample_map: dict[str, str] = field(init=False, default_factory=dict)\n",
    "\n",
    "    def expand_ranges(self, value):\n",
    "        '''\n",
    "        Given a value found in 'barcodes' column in samples.csv, return the expanded form, where no letters are skipped (ex: 1A-1G)\n",
    "        '''\n",
    "        expanded = []\n",
    "        for sub_string in value.split(';'):\n",
    "            if re.search('-', sub_string):\n",
    "                # Match the prefix and the range of letters (e.g., 1A-1C)\n",
    "                # Group 1: Number prefix (e.g., '1')\n",
    "                # Group 2: Start letter (e.g., 'A')\n",
    "                # Group 3: End letter (e.g., 'C')\n",
    "                match = re.match(r'(\\d+)([A-Z])-(\\d+)?([A-Z])', sub_string)\n",
    "                if match:\n",
    "                    prefix, start_char, _, end_char = match.groups()\n",
    "                    # Iterate through the ASCII values of the characters\n",
    "                    for char_code in range(ord(start_char), ord(end_char) + 1):\n",
    "                        expanded.append(f\"{prefix}{chr(char_code)}\")\n",
    "            else:\n",
    "                expanded.append(sub_string)\n",
    "        return ';'.join(expanded)\n",
    "\n",
    "    \n",
    "    def get_sample_ids_map(self):\n",
    "        '''\n",
    "        Given a lattice metadata and a samples.csv file, map the pseudosample and sample ids using 'RT_index' and 'barcodes' columns, respectively.\n",
    "        Need to split metadata by '-' and then swap plate row/column, and then also expand sample csv if there is shorthand (ex: 1A-1G,8A-8G)\n",
    "        '''\n",
    "        samples_csv_df = pd.read_csv(f'{DOWN_DIR}/samples.csv', index_col='sample')\n",
    "        metadata_df = self.metadata.metadata_df[['RT_index','pseudosample']].drop_duplicates().set_index('pseudosample')\n",
    "        \n",
    "        # After reading in csv and metadata df, parse barcode/RT_index so that they are comparable and store in dictionary\n",
    "        metadata_df['RT_transformed'] =  metadata_df['RT_index'].str.replace(r'[A-Za-z0-9]+-([A-H])([0-9]+)', r'\\2\\1', regex=True)\n",
    "        metadata_df['RT_transformed'] = metadata_df['RT_transformed'].apply(lambda x: ';'.join(sorted(x.split(','))) if isinstance(x, str) else x)\n",
    "        metadata_map = metadata_df['RT_transformed'].to_dict()\n",
    "        samples_csv_df['barcodes_transformed'] = samples_csv_df['barcodes'].apply(lambda x: self.expand_ranges(x))\n",
    "        samples_csv_df['barcodes_transformed'] = samples_csv_df['barcodes_transformed'].apply(lambda x: ';'.join(sorted(x.split(';'))) if isinstance(x, str) else x)\n",
    "        samples_csv_map = samples_csv_df['barcodes_transformed'].to_dict()\n",
    "\n",
    "        # Compare index values and return mapping of sample IDs\n",
    "        # This also checks that the samples.csv file is matching the metadata spreadsheet for wells\n",
    "        sample_map = {}\n",
    "        for pseudosample, value in metadata_map.items():\n",
    "            sample_ids = [k for k, v in samples_csv_map.items() if v==value]\n",
    "            if (len(sample_ids)>1):\n",
    "                print(f'ERROR: multimapping indices\\t{pseudosample}\\t{value}')\n",
    "            elif (len(sample_ids)==0):\n",
    "                print(f'ERROR: indices not matching\\t{pseudosample}\\t{value}')\n",
    "            else:\n",
    "                sample_map[pseudosample] = sample_ids[0]\n",
    "        self.sample_map = sample_map\n",
    "\n",
    "    \n",
    "    def download_files(self):\n",
    "        '''\n",
    "        Download all files needed for this experiment\n",
    "        Add samples.csv and allSamples.reportStatistics.csv to download, as these are needed for all experiments\n",
    "        '''\n",
    "        os.makedirs(DOWN_DIR, exist_ok=True)\n",
    "        self.file_dict[f'{self.uri.full_uri}samples.csv'] = f'{DOWN_DIR}/samples.csv'\n",
    "        self.file_dict[f'{self.uri.full_uri}reports/allSamples.reportStatistics.csv'] = f'{DOWN_DIR}/allSamples.reportStatistics.csv'\n",
    "        for s3_uri, local_path in self.file_dict.items():\n",
    "            if not os.path.exists(local_path):\n",
    "                print(f'Downloading from {s3_uri} to {local_path}')\n",
    "                fs.get(s3_uri, local_path)\n",
    "\n",
    "\n",
    "    def add_pseudosamples(self):\n",
    "        '''\n",
    "        Create all pseudosample object given sample_map and determine metadata for each.\n",
    "        Only load allcells for SingleSublibrary object\n",
    "        '''\n",
    "        for pseudo_id, samp_id in self.sample_map.items():\n",
    "            # First subset metadata df and map ontologies\n",
    "            print(f'Loading {samp_id}')\n",
    "            metadata_df_subset = self.metadata.metadata_df[self.metadata.metadata_df['pseudosample']==pseudo_id]\n",
    "            metadata_df_subset = map_ontologies(metadata_df_subset)\n",
    "            allcells_val = find_file(self.file_dict, samp_id, 'allCells.csv') if isinstance(self, SingleSublibrary) else None\n",
    "            \n",
    "            pseudosample = Pseudosample(\n",
    "                local_h5ad = find_file(self.file_dict, samp_id, 'h5ad'),\n",
    "                pseudosample_id = pseudo_id,\n",
    "                sample_id = samp_id,\n",
    "                allcells = allcells_val,\n",
    "                metadata_sample = metadata_df_subset,\n",
    "                parent = self\n",
    "            )\n",
    "            self.all_pseudosamples.append(pseudosample)\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.sample_map: dict = get_sample_ids_map()\n",
    "\n",
    "\n",
    "class SingleSublibrary(Experiment):\n",
    "    '''\n",
    "    Plate Experiment to store specific variables and methods for an experiment that is a single sublibrary,\n",
    "    which will include *allCell.csv files in addition to the sublibrary specific h5ad\n",
    "    '''\n",
    "    \n",
    "    def determine_files(self):\n",
    "        '''\n",
    "        Return the actual file URIs of files that need to be downloaded\n",
    "        '''\n",
    "        file_dict = {}\n",
    "        for file in self.uri.sample_files:\n",
    "            if file.endswith(('allCells.csv','_anndata.h5ad')):\n",
    "                filename = file.split('/')[-1]\n",
    "                local_path = os.path.join(DOWN_DIR, filename)\n",
    "                file_dict[file] = local_path\n",
    "        return file_dict\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.file_dict = self.determine_files()\n",
    "\n",
    "\n",
    "class MultiSublibrary(Experiment):\n",
    "    '''\n",
    "    Plate Experiment to store specific variables and methods for an experiment that has multiple sublibraries,\n",
    "    which will include the sample merged h5ad files\n",
    "    '''\n",
    "    def determine_files(self):\n",
    "        '''\n",
    "        Return the actual file URIs of files that need to be downloaded.\n",
    "        file_dict: dictionary where key is uri and value is local_path\n",
    "        '''\n",
    "        file_dict = {}\n",
    "        for file in self.uri.sample_files:\n",
    "            if file.endswith('anndata.h5ad') and not re.search('QSR', file):\n",
    "                filename = file.split('/')[-1]\n",
    "                local_path = os.path.join(DOWN_DIR, filename)\n",
    "                file_dict[file] = local_path\n",
    "        return file_dict\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.file_dict = self.determine_files()      \n",
    "\n",
    "\n",
    "def is_single(uripath_obj):\n",
    "    '''\n",
    "    Given an URI object, determine if the experiment is a single sublibrary experiment\n",
    "    by searching for 'QSR-#' for files on S3\n",
    "    '''\n",
    "    files = uripath_obj.sample_files\n",
    "    pattern = r'\\.(QSR-[1-8])_anndata.h5ad'\n",
    "    \n",
    "    sublibraries = list({re.search(pattern, f).group(1) for f in files if re.search(pattern, f)})\n",
    "    return len(sublibraries) == 1\n",
    "\n",
    "\n",
    "def find_file(file_dict, prefix, suffix):\n",
    "    '''\n",
    "    Given prefix and suffix, return file in file_dict\n",
    "    '''\n",
    "    matches = (f for f in list(file_dict.values())\n",
    "       if f.split('/')[-1].startswith(prefix) \n",
    "       and f.endswith(suffix))\n",
    "    return next(matches, None)\n",
    "\n",
    "\n",
    "def add_sample_metadata(pseudosamp):\n",
    "    '''\n",
    "    Given a Pseudosample object, add the sample level metadata from allSamples.reportStatistics.csv\n",
    "    '''\n",
    "    samples_df = pd.read_csv(f'{DOWN_DIR}/allSamples.reportStatistics.csv')\n",
    "    desired_rows = [\n",
    "        'Reads',\n",
    "        'Cells',\n",
    "        'Scaleplex'\n",
    "    ]\n",
    "    matches = (f for f in samples_df.columns\n",
    "       if f.split('/')[-1].startswith(pseudosamp.sample_id))\n",
    "    col = next(matches, None)\n",
    "    if col:\n",
    "        samples_df = samples_df[samples_df['Category'].isin(desired_rows)]\n",
    "        samples_df['new_index'] = samples_df['Category']+'_'+samples_df['Metric'].str.lower().str.replace(' ', '_')\n",
    "        samples_df.set_index('new_index', inplace=True)\n",
    "        samples_df = samples_df[[col]]\n",
    "        \n",
    "        pseudosamp.adata.obs['orig_index'] = pseudosamp.adata.obs.index\n",
    "        pseudosamp.adata.obs = pseudosamp.adata.obs.merge(samples_df.T, how='cross')\n",
    "        pseudosamp.adata.obs.set_index('orig_index', inplace=True)\n",
    "    else:\n",
    "        print(f'ERROR: cannot find sample metadata for\\t{pseudosamp.sample_id}')\n",
    "\n",
    "\n",
    "def cxg_add_labels(adata):\n",
    "    '''\n",
    "    Add ontology labels to adata.obs, and also fill in standard metadata fields in obs\n",
    "    '''\n",
    "    adata.uns['organism_ontology_term_id'] = adata.obs['organism_ontology_term_id'].unique()[0]\n",
    "    adata.obs.drop(columns=['organism_ontology_term_id'],inplace=True)\n",
    "    adata.obs['is_primary_data'] = True\n",
    "    \n",
    "    adata.obs['cell_type_ontology_term_id'] = 'unknown'\n",
    "    labeler = AnnDataLabelAppender(adata)\n",
    "    labeler._add_labels()\n",
    "    adata.obs.drop(columns=['cell_type_ontology_term_id','cell_type'],inplace=True)\n",
    "\n",
    "    schema_v = labeler.schema_version\n",
    "    adata.uns['schema_version'] = schema_v\n",
    "    adata.uns['schema_reference'] = labeler._build_schema_reference_url(schema_v)\n",
    "\n",
    "\n",
    "def add_guide_metadata(adata, sheet, guide_gid):\n",
    "    '''\n",
    "    Add guide metadata into adata.uns from Lattice wrangling sheet\n",
    "    \n",
    "    :param obj adata: the anndata object that is being transformed into the curated matrix\n",
    "    :param obj guide_df: the dataframe containing guide metadata from wrangling sheet\n",
    "    \n",
    "    :returns obj adata: modified adata to contain guide metadata\n",
    "    '''\n",
    "    url = f'https://docs.google.com/spreadsheets/d/{sheet}/export?format=csv&gid={guide_gid}'\n",
    "    response = requests.get(url)\n",
    "    guide_df = pd.read_csv(BytesIO(response.content), comment=\"#\", dtype=str)\n",
    "    genetic_perturbations = {}\n",
    "    \n",
    "    for row in guide_df.itertuples():\n",
    "        genetic_perturbations[row.guide_id] = {}\n",
    "        genetic_perturbations[row.guide_id]['role'] = 'targeting' if row.guide_role == 'Targeting a Gene' else 'control'\n",
    "        genetic_perturbations[row.guide_id]['protospacer_sequence'] = row.guide_protospacer\n",
    "        genetic_perturbations[row.guide_id]['protospacer_adjacent_motif'] = row.guide_PAM\n",
    "        if not pd.isna([row.start,row.end,row.strand]).all():\n",
    "            chr_loc = str(row.chromosome).replace(\"chr\",\"\") + \":\" + str(row.start) + \"-\" + str(row.end) + \"(\" + str(row.strand) + \")\"\n",
    "            genetic_perturbations[row.guide_id]['target_genomic_regions'] = [chr_loc]\n",
    "        if not pd.isna(row.overlapping_gene_ids):\n",
    "            genetic_perturbations[row.guide_id]['target_features'] = {}\n",
    "            for i in range(len(row.overlapping_gene_ids.split(\";\"))):\n",
    "                genetic_perturbations[row.guide_id]['target_features'][row.overlapping_gene_ids.split(\";\")[i]] = row.overlapping_gene_names.split(\";\")[i]\n",
    "                                                                             \n",
    "            \n",
    "    adata.uns['genetic_perturbations'] = genetic_perturbations\n",
    "    \n",
    "    return adata\n",
    "\n",
    "\n",
    "def determine_perturbation_strategy(adata):\n",
    "    '''\n",
    "    Assess feature_call from protospacer_calls_per_cell.csv, where if all guides\n",
    "    assigned to a single cell are all control, then 'control'. Otherwise, it is \"no perturbations\"\n",
    "    if no guids or one of the following if targeting:\n",
    "        - \"CRISPR activation screen\"\n",
    "        - \"CRISPR interference screen\"\n",
    "        - \"CRISPR knockout mutant\"\n",
    "        - \"CRISPR knockout screen\"\n",
    "    \n",
    "    :param obj adata: the anndata object that is being transformed into the curated matrix\n",
    "\n",
    "    :returns obj adata: modified adata to contain perturbation_strategy as cell metadata\n",
    "    '''\n",
    "    adata.obs['genetic_perturbation_strategy_calculated'] = adata.obs['genetic_perturbation_id']\n",
    "    adata.obs['genetic_perturbation_strategy_calculated'] = adata.obs['genetic_perturbation_strategy_calculated'].apply(\n",
    "        lambda x: x.split(' || ') if pd.notna(x) else 'no perturbations'\n",
    "    )\n",
    "    adata.obs['genetic_perturbation_strategy_calculated'] = adata.obs['genetic_perturbation_strategy_calculated'].apply(\n",
    "        lambda x: [adata.uns['genetic_perturbations'][i]['role'] for i in x] if isinstance(x, list)\n",
    "            else x        \n",
    "    )\n",
    "    adata.obs['genetic_perturbation_strategy_calculated'] = adata.obs['genetic_perturbation_strategy_calculated'].apply(\n",
    "         lambda x: 'control' if isinstance(x, list) and 'targeting' not in set(x)\n",
    "            else x\n",
    "    )\n",
    "    adata.obs.loc[adata.obs['genetic_perturbation_strategy_calculated']=='control', 'genetic_perturbation_strategy'] = 'control'\n",
    "    adata.obs.loc[adata.obs['genetic_perturbation_strategy_calculated']=='no perturbations', 'genetic_perturbation_strategy'] = 'no perturbations'\n",
    "    adata.obs.drop(columns=['genetic_perturbation_strategy_calculated'], inplace=True)\n",
    "    \n",
    "    return adata\n",
    "\n",
    "\n",
    "\n",
    "def map_ontologies(sample_df):\n",
    "    '''\n",
    "    Takes the sample metadata dataframe and standardizes ontologies\n",
    "    Also checks that standard fields are only filled out for appropriate organism\n",
    "\n",
    "    :param dataframe sample_df: the sample metadata from given google sheet\n",
    "\n",
    "    :returns dataframe sample_df: sample metadata with ontologies added\n",
    "    '''\n",
    "    col_ont_map = {\n",
    "        'organism':'NCBITaxon',\n",
    "        'sex':'PATO',\n",
    "        'self_reported_ethnicity':{'NCBITaxon:9606':'HANCESTRO',\n",
    "                                   'other':'none'},\n",
    "        'disease':'MONDO',\n",
    "        'assay':'EFO',\n",
    "        'development_stage':{'NCBITaxon:6239':'WBls', # C. Elegans\n",
    "                             'NCBITaxon:7227':'FBdv', # Drosophila\n",
    "                             'NCBITaxon:10090':'MmusDv', # Mouse\n",
    "                             'NCBITaxon:7955':'ZFS', # Zebrafish\n",
    "                             'other':'HsapDv' # For all other organisms, use HsapDv\n",
    "                            },\n",
    "        'tissue':{'NCBITaxon:6239':'WBbt', # C. Elegans\n",
    "                  'NCBITaxon:7227':'FBbt', # Drosophila\n",
    "                  'NCBITaxon:7955':'ZFA', # Zebrafish\n",
    "                  'other':'UBERON' # For all other organisms, use UBERON\n",
    "                 }\n",
    "    }\n",
    "    ontology_parser = OntologyParser()\n",
    "    ont_err_lst = []\n",
    "    \n",
    "    for col in col_ont_map:\n",
    "        map_dict = {}\n",
    "        for label in sample_df[col].unique():\n",
    "            term_id = None\n",
    "            if col == 'disease' and label == 'normal': # Normal is not in MONDO ontology\n",
    "                term_id = 'PATO:0000461'\n",
    "            elif label in ['unknown','na']: # Unknown and na won't be in ontologies, pass along\n",
    "                map_dict[label] = label\n",
    "                continue\n",
    "            elif col in ['tissue','development_stage','self_reported_ethnicity']:\n",
    "                if col == 'tissue':\n",
    "                    # Find what tissue type is at label row\n",
    "                    if sample_df.loc[sample_df[col] == label, 'tissue_type'].tolist()[0] != 'tissue':\n",
    "                        map_dict[label] = label # Don't map cell type in tissue\n",
    "                        continue\n",
    "                # Find what organism term id is at label row\n",
    "                org_term_id = sample_df.loc[sample_df[col] == label, 'organism_ontology_term_id'].tolist()[0]\n",
    "                if org_term_id in col_ont_map[col]:\n",
    "                    # Get ontology of specific organism and map label\n",
    "                    species_ont = col_ont_map[col][org_term_id]\n",
    "                    term_id = ontology_parser.get_term_id_by_label(label, species_ont)\n",
    "                else:\n",
    "                    if col_ont_map[col]['other'] == 'none':\n",
    "                        map_dict[label] = label\n",
    "                        continue\n",
    "                    else:\n",
    "                        term_id = ontology_parser.get_term_id_by_label(label, col_ont_map[col]['other'])\n",
    "            else:\n",
    "                term_id = ontology_parser.get_term_id_by_label(label, col_ont_map[col])\n",
    "            if term_id == None:\n",
    "                print(f'{label}\\t{col}\\t{term_id}\\t{org_term_id}')\n",
    "                if org_term_id:\n",
    "                    if org_term_id in col_ont_map[col]:\n",
    "                        ont_err_lst.append(f\"Error: Matching '{col_ont_map[col][org_term_id]}' term id not found for label '{label}' in column '{col}'\")\n",
    "                    else:\n",
    "                        ont_err_lst.append(f\"Error: Matching '{col_ont_map[col]['other']}' term id not found for label '{label}' in column '{col}'\")\n",
    "                else:\n",
    "                    ont_err_lst.append(f\"Error: Matching '{col_ont_map[col]}' term id not found for label '{label}' in column '{col}'\")\n",
    "                map_dict[label] = label\n",
    "                continue\n",
    "            map_dict[label] = term_id\n",
    "        sample_df[col + '_ontology_term_id'] = sample_df[col].map(map_dict)\n",
    "        del sample_df[col]\n",
    "    \n",
    "    ### Print out any errors from ontologizing\n",
    "    if ont_err_lst:\n",
    "        for e in ont_err_lst:\n",
    "            print(e)\n",
    "        sys.exit()\n",
    "\n",
    "    ### Convert string to boolean for is_pilot_data and donor_living_at_sample_collection\n",
    "    ### Check that donor_living_at_sample_collection is not filled out for non-human\n",
    "    b_type = ['is_pilot_data','donor_living_at_sample_collection']\n",
    "    for c in b_type:\n",
    "        if c in sample_df.columns:\n",
    "            if c == 'donor_living_at_sample_collection':\n",
    "                for val in sample_df[c].unique():\n",
    "                    if val != 'na' and sample_df.loc[sample_df[c] == val, \n",
    "                    'organism_ontology_term_id'].tolist()[0] != 'NCBITaxon:9606':\n",
    "                        print(f\"ERROR: donor_living_at_sample_collection for non-human data should be 'na' but '{val}' is present\")\n",
    "                        sys.exit()\n",
    "            sample_df[c] == sample_df[c].replace({'FALSE':False, 'TRUE':True})\n",
    "    \n",
    "    ### Blank fields in worksheet result in NaN values in dataframe, replacing these with na?\n",
    "    ### Could also replace with unknown for certain columns using fillna options?\n",
    "    sample_df.fillna('na', inplace=True)\n",
    "    sample_df.drop(columns=[c for c in sample_df.columns if c.startswith('!')], inplace=True)\n",
    "\n",
    "    return sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ad3c1-19cb-4da3-8954-68c4a658f36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/jovyan/lattice-tools/cellxgene_resources')\n",
    "from cellxgene_mods import *\n",
    "evaluate_obs_schema(adatas[0].obs, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25701f1c-9a9f-41e5-8dc7-b8e7237a3e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
