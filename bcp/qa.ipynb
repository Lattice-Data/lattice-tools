{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f24496-7133-4e5e-96d3-16c6ffffb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from qa_mods import *\n",
    "\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "paginator = s3client.get_paginator('list_objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d804db-fc52-4f06-a81b-84bedd8deb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose data source: 'manifest' or 's3'\n",
    "data_source = 's3'\n",
    "\n",
    "# === Common parameters (needed for both modes) ===\n",
    "order = ''      # Needed for output file naming\n",
    "raw_assay = ''  # 10x_viral_ORF, 10x, sci_jumbo, sci_plex, scale - needed for validation\n",
    "\n",
    "# === Parameters for 's3' mode only ===\n",
    "provider = ''  # psomagen, novogene\n",
    "proj = ''\n",
    "sub = '' #was this pipeline run as \"count\" or \"multi\"? for cellranger v9, this will be overwritten with whatever is in log\n",
    "\n",
    "# === Parameters for 'manifest' mode only ===\n",
    "manifest_path = ''        # Path to CSV/TSV manifest file\n",
    "manifest_delimiter = '\\t' # Use '\\t' for TSV, ',' for CSV\n",
    "manifest_s3_column = 0    # Column index containing S3 URIs (0-based)\n",
    "manifest_has_header = False  # Whether manifest has a header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf90ce5-9eec-4072-b516-5f72aa100ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Go through raw files to gather raw files and reports high level missing file log in *_errors.txt\n",
    "### Store metadata.json information in read_metadata\n",
    "\n",
    "fastq_log = {}\n",
    "all_raw_files = []\n",
    "all_proc_files = {}\n",
    "trimmer_failure_stats = {}\n",
    "read_metadata = {}\n",
    "\n",
    "output_dir = 'qa_outs'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if data_source == 'manifest':\n",
    "    # Load from manifest file\n",
    "    all_raw_files, all_proc_files = load_files_from_manifest(\n",
    "        manifest_path=manifest_path,\n",
    "        delimiter=manifest_delimiter,\n",
    "        s3_column=manifest_s3_column,\n",
    "        has_header=manifest_has_header\n",
    "    )\n",
    "    print(f\"Loaded {len(all_raw_files)} raw files from manifest\")\n",
    "    print(f\"Loaded {sum(len(v) for v in all_proc_files.values())} processed files across {len(all_proc_files)} groups\")\n",
    "    \n",
    "    # Note: When using manifest mode, the following are NOT available:\n",
    "    # - fastq_log (requires parsing each file)\n",
    "    # - trimmer_failure_stats (requires downloading and parsing trimmer files)\n",
    "    # - read_metadata (requires downloading metadata JSON files)\n",
    "    # These would require S3 access to populate.\n",
    "\n",
    "elif data_source == 's3':\n",
    "    bucket = f'czi-{provider}'\n",
    "\n",
    "    ### Gathering all the raw files\n",
    "    o = f'{proj}/{order}/'\n",
    "    r_order = s3client.list_objects(Bucket=bucket, Prefix=o, Delimiter='/')\n",
    "    if 'CommonPrefixes' in r_order:\n",
    "        groups = [e['Prefix'] for e in r_order['CommonPrefixes']]\n",
    "        for g in groups:\n",
    "            fastq_log[g.replace(o, '').rstrip('/')] = {}\n",
    "            r_group = s3client.list_objects(Bucket=bucket, Prefix=g, Delimiter='/')\n",
    "            subdirs = [e['Prefix'] for e in r_group['CommonPrefixes']]\n",
    "            if len(subdirs) > 2:\n",
    "                print('EXTRA subdirs', s)\n",
    "            if f'{g}raw/' not in subdirs:\n",
    "                print('raw/ MISSING', g)\n",
    "            else:\n",
    "                r_raw = s3client.list_objects(Bucket=bucket, Prefix=f'{g}raw/', Delimiter='/')\n",
    "                if 'CommonPrefixes' in r_raw:\n",
    "                    non_10x_runs = [e['Prefix'] for e in r_raw['CommonPrefixes']]\n",
    "                    raw_files = []\n",
    "                    for run in non_10x_runs:\n",
    "                        r_run = s3client.list_objects(Bucket=bucket, Prefix=run, Delimiter='/')\n",
    "                        if 'Contents' in r_run:\n",
    "                            content = [c['Key'] for c in r_run['Contents']]\n",
    "                            raw_files.extend([c['Key'] for c in r_run['Contents']])\n",
    "                            tenX = False\n",
    "                if 'Contents' in r_raw:\n",
    "                    raw_files = [c['Key'] for c in r_raw['Contents']]\n",
    "                    tenX = True\n",
    "                if raw_files:\n",
    "                    all_raw_files.extend(raw_files)\n",
    "                    for rf in raw_files:\n",
    "                        parsed = parse_raw_filename(rf, raw_assay)\n",
    "                        if parsed is not None:\n",
    "                            run,group,assay,ug,barcode = parsed\n",
    "                            if assay not in valid_assays:\n",
    "                                e = f'WRONG ASSAY: {assay} {rf}'\n",
    "                                with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "                                    file.write(e + '\\n')\n",
    "                            if group != g.replace(o, '').rstrip('/') and tenX:\n",
    "                                e = f\"WRONG GROUP: {group} {g.replace(o, '').rstrip('/')} {rf}\"\n",
    "                                with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "                                    file.write(e + '\\n')\n",
    "                            if (rf.endswith('.fastq.gz') and not rf.endswith('_sample.fastq.gz')) or \\\n",
    "                                (rf.endswith('.cram') and assay == 'viral_ORF') or \\\n",
    "                                (rf.endswith('.cram') and raw_assay == 'scale' and not rf.endswith('_unmatched.cram')):\n",
    "                                if group not in fastq_log:\n",
    "                                    fastq_log[group] = {}\n",
    "                                if assay in fastq_log[group]:\n",
    "                                    fastq_log[group][assay].append(rf.split('/')[-1])\n",
    "                                else:\n",
    "                                    fastq_log[group][assay] = [rf.split('/')[-1]]\n",
    "\n",
    "                        ### Go through metadata.json and trimmer logs and store information\n",
    "                        if rf.endswith('fastq.gz-metadata.json') and not rf.endswith('_sample.fastq.gz-metadata.json'):\n",
    "                            s3client.download_file(bucket, rf, 'metadata.json')\n",
    "                            metadata = json.load(open('metadata.json'))\n",
    "                            read_metadata[metadata['filename']] = metadata\n",
    "                            os.remove('metadata.json')\n",
    "                        elif rf.endswith('trimmer-failure_codes.csv') and not rf.endswith('merged_trimmer-failure_codes.csv'):\n",
    "                            grab_trimmer_stats(trimmer_failure_stats, rf, bucket)\n",
    "\n",
    "            ### Go through processed files\n",
    "            if f'{g}processed/' not in subdirs:\n",
    "                print('processed/ MISSING', g)\n",
    "            else:\n",
    "                r_proc = s3client.list_objects(Bucket=bucket, Prefix=f'{g}processed/', Delimiter='/')\n",
    "                if 'Contents' in r_proc:\n",
    "                    print('UNEXPECTED FILES', f'{s}processed/')\n",
    "                if f'{g}processed/cellranger/' not in [e['Prefix'] for e in r_proc['CommonPrefixes']]:\n",
    "                    print('cellranger/ MISSING', g)\n",
    "                else:\n",
    "                    r_cr = s3client.list_objects(Bucket=bucket, Prefix=f'{g}processed/cellranger/', Delimiter='/')\n",
    "                    if 'Contents' in r_cr:\n",
    "                        print('UNEXPECTED FILES', f'{g}processed/cellranger/')\n",
    "                    run_dates = [e['Prefix'] for e in r_cr['CommonPrefixes']]\n",
    "                    for rd in run_dates:\n",
    "                        date = rd.split('/')[-2]\n",
    "                        if date[:7] != 'Run_202' or date[7] not in ['5'] or date[8] != '-' or date[11] != '-' or int(date[9:11]) > 12 or int(date[12:14]) > 31 or len(date) != 14:\n",
    "                            e = f'INCORRECT DATE FORMAT: {date} {rd}'\n",
    "                            with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "                                file.write(e + '\\n')\n",
    "                        r_date = s3client.list_objects(Bucket=bucket, Prefix=rd, Delimiter='/')\n",
    "                        if 'Contents' in r_date:\n",
    "                            for c in r_date['Contents']:\n",
    "                                print('UNEXPECTED FILES', c['Key'])\n",
    "                        outsdirs = [e['Prefix'] for e in r_date['CommonPrefixes']]\n",
    "                        if len(outsdirs) > 1:\n",
    "                            print('EXTRA subdirs', rd)\n",
    "                        if f'{rd}outs/' not in outsdirs:\n",
    "                            print('NO outs/', rd)\n",
    "                        else:\n",
    "                            files = []\n",
    "                            for page in paginator.paginate(Bucket=bucket, Prefix=f'{rd}outs/'):\n",
    "                                if 'Contents' in page:\n",
    "                                    for obj in page['Contents']:\n",
    "                                        files.append(obj['Key'])\n",
    "                            all_proc_files[g.replace(o, '').rstrip('/')] = files\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Invalid data_source: {data_source}. Must be 'manifest' or 's3'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23407a-89ac-435f-bf99-697cbbf06c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots distribution of frequency of sequencing and trimming fail rates\n",
    "### One plot per sample, where each single trimmer-failure_codes.csv and merged_trimmer-failure_codes.csv per wafer is considered to be a single value\n",
    "\n",
    "for exp in trimmer_failure_stats.keys():\n",
    "    plt.hist(trimmer_failure_stats[exp]['rsq'], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.ylabel('Frequence')\n",
    "    plt.xlabel('Percent fail sequencing (rsq file)')\n",
    "    plt.title(f'Distribution Failed Sequencing {exp}')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(trimmer_failure_stats[exp]['trimmer_fail'], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.ylabel('Frequence')\n",
    "    plt.xlabel('Percent fail trimming')\n",
    "    plt.title(f'Distribution Failed Trimming {exp}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47808632-5c0c-42f3-9878-3250f7766334",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for mismatching number of files for multiple modalities\n",
    "\n",
    "for sample,v in fastq_log.items():\n",
    "    if raw_assay == 'scale':\n",
    "        if len(fastq_log[sample]['GEX']) != len(fastq_log[sample]['hash_oligo']):\n",
    "            e = f\"MISMATCH FQ COUNTS: {sample}: {len(v.get('GEX',[]))} GEX, {len(v.get('hash_oligo',[]))} hash_oligo\"\n",
    "            with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "                file.write(e + '\\n')\n",
    "    if len(v.get('CRI',[])) != len(v.get('GEX',[])) and len(v.get('GEX',[]))/2 != len(v.get('viral_ORF',[])):\n",
    "        e = f\"MISMATCH FQ COUNTS: {sample}: {len(v.get('CRI',[]))} CRI, {len(v.get('GEX',[]))} GEX, {len(v.get('viral_ORF',[]))} viral_ORF\"\n",
    "        with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "            file.write(e + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfafc2c-f4de-484f-8605-4bfca95dbb9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Check for mismatching read counts between R1 and R2 for same run and appends to *_errors.txt\n",
    "### If there is at least 1 error in the metadata.json, read_count is not evaluated\n",
    "### Needs to be updated for Multiome\n",
    "\n",
    "error = ''\n",
    "\n",
    "group_read_counts = {}\n",
    "for f,meta in read_metadata.items():\n",
    "    if '_R2_' not in f:\n",
    "        if meta['errors']!=[]:\n",
    "            print(f'WARNING: ERROR in metadata.json log for {f}')\n",
    "            error+=f'METADATA.JSON ERROR: {f} has error in metadata.json:{meta[\"errors\"]}\\n'\n",
    "            continue\n",
    "        reads = meta['read_count']\n",
    "        parsed = parse_raw_filename(f, raw_assay)\n",
    "        if parsed is not None:\n",
    "            run,group,assay,ug,barcode = parsed\n",
    "            if group not in group_read_counts:\n",
    "                group_read_counts[group] = {assay: reads}\n",
    "            elif assay not in group_read_counts[group]:\n",
    "                group_read_counts[group][assay] = reads\n",
    "            else:\n",
    "                group_read_counts[group][assay] += reads\n",
    "        if '_R1_' in f:\n",
    "            r2file = f.replace('_R1_','_R2_')\n",
    "            if r2file in read_metadata:\n",
    "                if read_metadata[r2file]['errors']!=[]:\n",
    "                    print(f'WARNING: ERROR in metadata.json log for {f}')\n",
    "                    error+=f'METADATA.JSON ERROR: {r2file} has error in metadata.json:{read_metadata[r2file][\"errors\"]}\\n'\n",
    "                    continue\n",
    "                r2reads = read_metadata[r2file].get('read_count')\n",
    "                if reads != r2reads:\n",
    "                    error = f'READ COUNT ERROR:{f}-{reads},{r2file}-{r2reads}\\n'\n",
    "                else:\n",
    "                    print(f'GOOD: Matching read counts R1 and R2: {reads} {r2reads}')\n",
    "        with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "            file.write(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546805a0-d555-4d95-95ad-15342fc8db31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Checking for expected files for raw, compiling list of \"beginnings\" and \"endings\" of filenames\n",
    "### Will need to update the logging for \"all good\", it is all files for that \"beginning\" is all present\n",
    "\n",
    "beginnings = {}\n",
    "for fullpath in all_raw_files:\n",
    "    parsed = parse_raw_filename(fullpath, raw_assay)\n",
    "    if parsed is not None:\n",
    "        run,group,assay,ug,barcode = parsed\n",
    "        b = f'{run}-{group}_{assay}-{ug}-{barcode}'\n",
    "        if b not in beginnings:\n",
    "            raw_dir = '/'.join(fullpath.split('/')[:-1])\n",
    "            endings = raw_expected[raw_assay]\n",
    "            if raw_assay == '10x_viral_ORF' and assay == 'GEX':\n",
    "                endings = raw_expected['10x']\n",
    "            beginnings[b] = {\n",
    "                'raw_dir': raw_dir,\n",
    "                'endings': endings\n",
    "            }\n",
    "\n",
    "all_good = 0\n",
    "raw_lost = []\n",
    "raw_found = []\n",
    "for b,v in beginnings.items():\n",
    "    temp_missing = {'path': b}\n",
    "    for e in v['endings']:\n",
    "        f = f\"{v['raw_dir']}/{b}{e}\"\n",
    "        if f not in all_raw_files:\n",
    "            if e.endswith('-metadata.json') and f.replace('-metadata.json','') not in all_raw_files:\n",
    "                continue\n",
    "            temp_missing[e] = f\n",
    "        else:\n",
    "            raw_found.append(f)\n",
    "    if len(temp_missing) > 1:\n",
    "        raw_lost.append(temp_missing)\n",
    "    else:\n",
    "        all_good += 1\n",
    "print(f'{all_good} out of {len(beginnings)} are GOOD and do not have missing raw files')\n",
    "df = pd.DataFrame(raw_lost)\n",
    "if not df.empty:\n",
    "    print(\"WARNING: There are missing raw files that are expected\")\n",
    "    df.to_csv(f'{output_dir}/{order}_raw_missing.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590163d6-9e7d-4bce-8746-19eecb75d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking for extra fastq files, and there is a set of \"optional\" raw files are commonly missing \n",
    "\n",
    "for f in all_raw_files:\n",
    "    if f in raw_found:\n",
    "        continue\n",
    "    if f.endswith('-metadata.json') and f.replace('-metadata.json','') in all_raw_files:\n",
    "        continue\n",
    "    if raw_assay in raw_optional or raw_assay == '10x_viral_ORF':\n",
    "        endings = raw_optional.get(raw_assay,[])\n",
    "        if raw_assay == '10x_viral_ORF' and assay == 'GEX':\n",
    "            endings = raw_optional['10x']\n",
    "        parsed = parse_raw_filename(f, raw_assay)\n",
    "        if parsed is not None:\n",
    "            run,group,assay,ug,barcode = parsed\n",
    "            b = f'{run}-{group}_{assay}-{ug}-{barcode}'\n",
    "            raw_dir = '/'.join(f.split('/')[:-1])\n",
    "            if f.replace(f'{raw_dir}/{b}','') in endings:\n",
    "                continue\n",
    "    with open(f'{output_dir}/{order}_raw_extra.txt', 'a') as file:\n",
    "        file.write(f + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272b13c-ca74-42d9-8c8c-295d21b267ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Go through cellranger logs and check for expected run metadata and presence of files\n",
    "### Will need to update for Multiome\n",
    "\n",
    "proc_missing = []\n",
    "alerts = []\n",
    "for g,proc_files in all_proc_files.items():\n",
    "    report = {}\n",
    "\n",
    "    web_summ = False\n",
    "    while not web_summ:\n",
    "        for f in proc_files:\n",
    "            if f.split('/')[-1] == 'web_summary.html':\n",
    "                s3client.download_file(f'czi-{provider}', f, 'web_summary.html')\n",
    "                report.update(parse_web_summ('web_summary.html'))\n",
    "                os.remove('web_summary.html')\n",
    "                web_summ = True\n",
    "                break\n",
    "\n",
    "    met_summ = False\n",
    "    while not met_summ:\n",
    "        for f in proc_files:\n",
    "            if f.split('/')[-1] == 'metrics_summary.csv':\n",
    "                s3client.download_file(f'czi-{provider}', f, 'metrics_summary.csv')\n",
    "                report.update(parse_met_summ('metrics_summary.csv'))\n",
    "                os.remove('metrics_summary.csv')\n",
    "                met_summ = True\n",
    "                break\n",
    "\n",
    "    \n",
    "    chem = report['chem']\n",
    "    extra = report['extra']\n",
    "    software = report['software']\n",
    "\n",
    "\n",
    "    if software == 'cellranger-9.0.1':\n",
    "        sub = report['sub']\n",
    "    elif software == 'cellranger-10.0.0':\n",
    "        sub = 'multi'    \n",
    "    else:\n",
    "        e = f'CR ERROR: {g} version is {software} but should be 9.0.1 or 10.0.0\\n'\n",
    "        with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "            file.write(e)\n",
    "\n",
    "\n",
    "    if 'min-crispr-umi' in report:\n",
    "        cri_umi = report['min-crispr-umi']\n",
    "        if cri_umi != '3':\n",
    "            e = f'CR ERROR: {g} min-crispr-umi is {cri_umi} but should be 3\\n'\n",
    "            with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "                file.write(e)\n",
    "\n",
    "    if 'incl_int' in report:\n",
    "        intron = report['incl_int']\n",
    "        if intron != 'true':\n",
    "            e = f'CR ERROR: {g} include-introns is {intron} but should be true\\n'\n",
    "            with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "                file.write(e)\n",
    "\n",
    "    if 'create-bam' in report:\n",
    "        bam = report['create-bam']\n",
    "        if chem != 'flex' and bam != 'true':\n",
    "            e = f'CR ERROR: {g} create-bam is {bam} but should be true\\n'\n",
    "            with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "                file.write(e)\n",
    "\n",
    "    for a in report['gex_alerts']:\n",
    "        alert = {'group': g, 'modality': 'GEX'}\n",
    "        alert.update(a)\n",
    "        alerts.append(alert)\n",
    "    for a in report.get('crispr_alerts',[]):\n",
    "        alert = {'group': g, 'modality': 'CRI'}\n",
    "        alert.update(a)\n",
    "        alerts.append(alert)\n",
    "\n",
    "    if sub == 'multi':\n",
    "        if chem == 'flex':\n",
    "            expected = cellranger_expected[software]['flex']['outs'].copy()\n",
    "            per_samp_expected = cellranger_expected[software]['flex']['per_sample'].copy()\n",
    "        else:\n",
    "            expected = cellranger_expected[software]['nonflex']['outs'].copy()\n",
    "            per_samp_expected = cellranger_expected[software]['nonflex']['per_sample'].copy()\n",
    "    elif sub == 'count':\n",
    "        expected = cellranger_expected['count']['outs'].copy()\n",
    "        per_samp_expected = []\n",
    "\n",
    "    #https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/outputs/cr-outputs-ab-overview\n",
    "    #https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/outputs/cr-outputs-crispr-overview\n",
    "    #https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/running-pipelines/cr-cell-annotation-pipeline\n",
    "    if 'CRISPR' in extra or 'Antibody' in extra:\n",
    "        if software!='cellranger-10.0.0':\n",
    "            expected.append('multi/count/feature_reference.csv')\n",
    "            per_samp_expected.append('count/feature_reference.csv')\n",
    "        if 'CRISPR' in extra and software!='cellranger-10.0.0':\n",
    "            per_samp_expected.append('count/crispr_analysis.tar.gz')\n",
    "        if 'Antibody' in extra:\n",
    "            per_samp_expected.append('count/antibody_analysis.tar.gz')\n",
    "    if 'CellAnnotate' in extra:\n",
    "        per_samp_expected.append('count/cell_types.tar.gz')\n",
    "\n",
    "    if sub == 'multi' and report.get('multiplex'):\n",
    "        if software!='cellranger-10.0.0':\n",
    "            expected.append('multi/multiplexing_analysis.tar.gz')\n",
    "    \n",
    "    actual = [f.split('/outs/', 1)[1] for f in proc_files if f.split('/')[-1] != 'curated.h5ad']\n",
    "    per_samp_actual = [f for f in actual if f.startswith('per_sample_outs/')]\n",
    "    missing = [f for f in expected if f not in actual]\n",
    "    if missing:\n",
    "        temp_missing = {'group': g}\n",
    "        for m in missing:\n",
    "            temp_missing[m] = 'Y'\n",
    "        proc_missing.append(temp_missing)\n",
    "\n",
    "    if per_samp_expected:\n",
    "        samples = list(set([f.split('/')[8] for f in proc_files if f.split('/')[7] == 'per_sample_outs']))\n",
    "        for s in samples:\n",
    "            expected_samp = [f'per_sample_outs/{s}/{f}' for f in per_samp_expected]\n",
    "            expected.extend(expected_samp)\n",
    "            missing_samp = [f for f in expected_samp if f not in actual]\n",
    "            if missing_samp:\n",
    "                temp_missing = {'group': f'{g}/{s}'}\n",
    "                for m in missing_samp:\n",
    "                    temp_missing[m.replace(f'per_sample_outs/{s}/','')] = 'Y'\n",
    "                proc_missing.append(temp_missing)                \n",
    "\n",
    "    extra = [f for f in actual if f not in expected and not f.endswith('manifest.json')]\n",
    "    if extra:\n",
    "        with open(f'{output_dir}/{order}_process_extra.txt', 'a') as file:\n",
    "            file.write(g + '\\n')\n",
    "            file.write(','.join(extra) + '\\n' + '\\n')\n",
    "\n",
    "    ### Reads per assay and comparing to metadata.json count\n",
    "    for k,v in report.items():\n",
    "        if k.endswith('_reads'):\n",
    "            assay = k.replace('_reads','')\n",
    "            if g in group_read_counts:\n",
    "                if assay in group_read_counts[g]:\n",
    "                    v2 = group_read_counts[g][assay]\n",
    "                    if v != v2:\n",
    "                        e = f'READ COUNT ERROR: {g} {assay} {v} from proc,{v2} from raw,{v - v2} diff\\n'\n",
    "                        with open(f'{output_dir}/{order}_errors.txt', 'a') as file:\n",
    "                            file.write(e)\n",
    "                    else:\n",
    "                        print(f'GOOD: Matching total read counts: {v} {v2}')\n",
    "                else:\n",
    "                    ### Could not find the group or the assay for this group\n",
    "                    print('WARNING: Read count not found in metadata.json for',g,assay,'\\n')\n",
    "            else:\n",
    "                print('WARNING: Read count for found in metadata.json for',g,'\\n')\n",
    "\n",
    "if alerts:\n",
    "    pd.DataFrame(alerts).to_csv(f'{output_dir}/{order}_process_alerts.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(proc_missing)\n",
    "if not df.empty:\n",
    "    print(f'\\nWARNING: Table of missing items')\n",
    "    df.to_csv(f'{output_dir}/{order}_process_missing.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c238630-5a61-4a7c-b03c-9c46a4bacfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
