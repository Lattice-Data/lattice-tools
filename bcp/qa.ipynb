{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f24496-7133-4e5e-96d3-16c6ffffb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from qa_mods import *\n",
    "\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "paginator = s3client.get_paginator('list_objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d804db-fc52-4f06-a81b-84bedd8deb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = 'psomagen'\n",
    "\n",
    "proj = 'weissman-scaling-in-vivo-perturb-seq-in-the-liver-and-beyond/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf90ce5-9eec-4072-b516-5f72aa100ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastq_log = {}\n",
    "all_raw_files = []\n",
    "all_proc_files = {}\n",
    "trimmer_failure_stats = {}\n",
    "read_metadata = {}\n",
    "\n",
    "bucket = f'czi-{provider}'\n",
    "\n",
    "r_proj = s3client.list_objects(Bucket=bucket, Prefix=proj, Delimiter='/')\n",
    "orders = [e['Prefix'] for e in r_proj['CommonPrefixes']]\n",
    "\n",
    "for o in orders:\n",
    "    r_order = s3client.list_objects(Bucket=bucket, Prefix=o, Delimiter='/')\n",
    "    if 'CommonPrefixes' in r_order:\n",
    "        groups = [e['Prefix'] for e in r_order['CommonPrefixes']]\n",
    "        for g in groups:\n",
    "            fastq_log[g.replace(o, '').rstrip('/')] = {}\n",
    "            r_group = s3client.list_objects(Bucket=bucket, Prefix=g, Delimiter='/')\n",
    "            subdirs = [e['Prefix'] for e in r_group['CommonPrefixes']]\n",
    "            if len(subdirs) > 2:\n",
    "                print('EXTRA subdirs', s)\n",
    "            if f'{g}raw/' not in subdirs:\n",
    "                print('raw MISSING', g)\n",
    "            else:\n",
    "                r_raw = s3client.list_objects(Bucket=bucket, Prefix=f'{g}raw/', Delimiter='/')\n",
    "                if 'CommonPrefixes' in r_raw:\n",
    "                    print('EXTRA subdirs', g)\n",
    "                    non_10x_runs = [e['Prefix'] for e in r_raw['CommonPrefixes']]\n",
    "                    for run in non_10x_runs:\n",
    "                        r_run = s3client.list_objects(Bucket=bucket, Prefix=run, Delimiter='/')\n",
    "                        if 'Contents' in r_run:\n",
    "                            raw_files = [c['Key'] for c in r_run['Contents']]\n",
    "                            tenX = False\n",
    "                if 'Contents' in r_raw:\n",
    "                    raw_files = [c['Key'] for c in r_raw['Contents']]\n",
    "                    tenX = True\n",
    "                if raw_files:\n",
    "                    all_raw_files.extend(raw_files)\n",
    "                    for rf in raw_files:\n",
    "                        run,group,assay = parse_raw_filename(rf)\n",
    "\n",
    "                        if assay not in valid_assays:\n",
    "                            print('WRONG assay',assay,rf)\n",
    "                        if group != g.replace(o, '').rstrip('/'):\n",
    "                            print('WRONG group',group,g.replace(o, '').rstrip('/'),rf)\n",
    "                        if (rf.endswith('.fastq.gz') and not rf.endswith('_sample.fastq.gz')) or (rf.endswith('.cram') and assay == 'viral_ORF'):\n",
    "                            if assay in fastq_log[group]:\n",
    "                                fastq_log[group][assay].append(rf.split('/')[-1])\n",
    "                            else:\n",
    "                                fastq_log[group][assay] = [rf.split('/')[-1]]\n",
    "\n",
    "                        if rf.endswith('fastq.gz-metadata.json') and not rf.endswith('_sample.fastq.gz-metadata.json'):\n",
    "                            s3client.download_file(bucket, rf, 'metadata.json')\n",
    "                            metadata = json.load(open('metadata.json'))\n",
    "                            read_metadata[metadata['filename']] = metadata\n",
    "                            os.remove('metadata.json')\n",
    "                        elif rf.endswith('trimmer-failure_codes.csv') and not rf.endswith('merged_trimmer-failure_codes.csv'):\n",
    "                            grab_trimmer_stats(trimmer_failure_stats, rf, bucket)\n",
    "\n",
    "            if f'{g}processed/' not in subdirs:\n",
    "                print('processed MISSING', g)\n",
    "            else:\n",
    "                r_proc = s3client.list_objects(Bucket=bucket, Prefix=f'{g}processed/', Delimiter='/')\n",
    "                if 'Contents' in r_proc:\n",
    "                    print('UNEXPECTED FILES', f'{s}processed/')\n",
    "                if f'{g}processed/cellranger/' not in [e['Prefix'] for e in r_proc['CommonPrefixes']]:\n",
    "                    print('cellranger MISSING', g)\n",
    "                else:\n",
    "                    r_cr = s3client.list_objects(Bucket=bucket, Prefix=f'{g}processed/cellranger/', Delimiter='/')\n",
    "                    if 'Contents' in r_cr:\n",
    "                        print('UNEXPECTED FILES', f'{g}processed/cellranger/')\n",
    "                    run_dates = [e['Prefix'] for e in r_cr['CommonPrefixes']]\n",
    "                    for rd in run_dates:\n",
    "                        date = rd.split('/')[-2]\n",
    "                        if date[:7] != 'Run_202' or date[7] not in ['5'] or date[8] != '-' or date[11] != '-' or int(date[9:11]) > 12 or int(date[12:14]) > 31 or len(date) != 14:\n",
    "                            print('INCORRECT date format',date,rd)\n",
    "                        r_date = s3client.list_objects(Bucket=bucket, Prefix=rd, Delimiter='/')\n",
    "                        if 'Contents' in r_date:\n",
    "                            for c in r_date['Contents']:\n",
    "                                print('UNEXPECTED FILES', c['Key'])\n",
    "                        outsdirs = [e['Prefix'] for e in r_date['CommonPrefixes']]\n",
    "                        if len(outsdirs) > 1:\n",
    "                            print('TOO MANY OUTS', rd)\n",
    "                        elif f'{rd}outs/' not in outsdirs:\n",
    "                            print('NO OUTS', rd)\n",
    "                        else:\n",
    "                            files = []\n",
    "                            for page in paginator.paginate(Bucket=bucket, Prefix=f'{rd}outs/'):\n",
    "                                if 'Contents' in page:\n",
    "                                    for obj in page['Contents']:\n",
    "                                        files.append(obj['Key'])\n",
    "                            all_proc_files[g.replace(o, '').rstrip('/')] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23407a-89ac-435f-bf99-697cbbf06c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram per order\n",
    "for exp in trimmer_failure_stats.keys():\n",
    "    plt.hist(trimmer_failure_stats[exp]['rsq'], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.ylabel('Frequence')\n",
    "    plt.xlabel('Percent fail sequencing (rsq file)')\n",
    "    plt.title(f'Distribution Failed Sequencing {exp}')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(trimmer_failure_stats[exp]['trimmer_fail'], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.ylabel('Frequence')\n",
    "    plt.xlabel('Percent fail trimming')\n",
    "    plt.title(f'Distribution Failed Trimming {exp}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47808632-5c0c-42f3-9878-3250f7766334",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample,v in fastq_log.items():\n",
    "    if len(v.get('CRI',[])) != len(v.get('GEX',[])) and len(v.get('GEX',[]))/2 != len(v.get('viral_ORF',[])):\n",
    "        print(f\"MISMATCH FQ counts: {sample}: {len(v.get('CRI',[]))} CRI, {len(v.get('GEX',[]))} GEX, {len(v.get('viral_ORF',[]))} viral_ORF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfafc2c-f4de-484f-8605-4bfca95dbb9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beginnings = {}\n",
    "for fullpath in all_raw_files:\n",
    "    components = fullpath.split('/')[-1].split('-')\n",
    "    if len(components) > 3:\n",
    "        run = components[0]\n",
    "        group_assay = components[1]\n",
    "        ug = components[2]\n",
    "        barcode = components[3].split('_')[0].split('.')[0]\n",
    "        b = f'{run}-{group_assay}-{ug}-{barcode}'\n",
    "        if b not in beginnings:\n",
    "            raw_dir = '/'.join(fullpath.split('/')[:-1])\n",
    "    \n",
    "            if group_assay.endswith('viral_ORF'):\n",
    "                endings = raw_expected['viralORF']\n",
    "            else:\n",
    "                endings = raw_expected['standard']\n",
    "    \n",
    "            beginnings[b] = {\n",
    "                'raw_dir': raw_dir,\n",
    "                'endings': endings\n",
    "            }\n",
    "\n",
    "raw_lost = []\n",
    "for b,v in beginnings.items():\n",
    "    temp_missing = {'path': b}\n",
    "    for e in v['endings']:\n",
    "        f = f\"{v['raw_dir']}/{b}{e}\"\n",
    "        if f not in all_raw_files:\n",
    "            temp_missing[e] = f\n",
    "        else:\n",
    "            all_raw_files.remove(f)\n",
    "    if len(temp_missing) > 1:\n",
    "        raw_lost.append(temp_missing)\n",
    "pd.DataFrame(raw_lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546805a0-d555-4d95-95ad-15342fc8db31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group_read_counts = {}\n",
    "r1r2_errors = []\n",
    "for f,meta in read_metadata.items():\n",
    "    if '_R1_' in f:\n",
    "        run,group,assay = parse_raw_filename(f)\n",
    "        r1_reads = meta['read_count']\n",
    "\n",
    "        if group in group_read_counts:\n",
    "            if assay in group_read_counts[group]:\n",
    "                group_read_counts[group][assay] += r1_reads\n",
    "            else:\n",
    "                group_read_counts[group][assay] = r1_reads\n",
    "        else:\n",
    "            group_read_counts[group] = {assay: r1_reads}\n",
    "\n",
    "        r2_file = f.replace('_R1_','_R2_')\n",
    "        if r2_file not in read_metadata:\n",
    "            r2_reads = 'n/a'\n",
    "        else:\n",
    "            r2_reads = read_metadata[r2_file]['read_count']\n",
    "        if r1_reads != r2_reads:\n",
    "            r1r2_errors.append({\n",
    "                'R1 file': f,\n",
    "                'R1 reads': r1_reads,\n",
    "                'R2 file': r2_file,\n",
    "                'R2 reads': r2_reads,\n",
    "            })\n",
    "\n",
    "    #add read_length metric here\n",
    "\n",
    "pd.DataFrame(r1r2_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590163d6-9e7d-4bce-8746-19eecb75d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are 'extra' files\n",
    "for f in all_raw_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272b13c-ca74-42d9-8c8c-295d21b267ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proc_missing = []\n",
    "for g,proc_files in all_proc_files.items():\n",
    "    report = {}\n",
    "\n",
    "    web_summ = False\n",
    "    while not web_summ:\n",
    "        for f in proc_files:\n",
    "            if f.split('/')[-1] == 'web_summary.html':\n",
    "                s3client.download_file(f'czi-{provider}', f, 'web_summary.html')\n",
    "                report.update(parse_web_summ('web_summary.html'))\n",
    "                os.remove('web_summary.html')\n",
    "                web_summ = True\n",
    "                break\n",
    "\n",
    "    met_summ = False\n",
    "    while not met_summ:\n",
    "        for f in proc_files:\n",
    "            if f.split('/')[-1] == 'metrics_summary.csv':\n",
    "                s3client.download_file(f'czi-{provider}', f, 'metrics_summary.csv')\n",
    "                report.update(parse_met_summ('metrics_summary.csv'))\n",
    "                os.remove('metrics_summary.csv')\n",
    "                met_summ = True\n",
    "                break\n",
    "\n",
    "    sub = report['sub']\n",
    "    chem = report['chem']\n",
    "    extra = report['extra']\n",
    "    software = report['software']\n",
    "\n",
    "    if software != 'cellranger-9.0.1':\n",
    "        print(f'ERROR {g}:version is {software} but should be 9.0.1\\n')\n",
    "\n",
    "    if 'min-crispr-umi' in report:\n",
    "        cri_umi = report['min-crispr-umi']\n",
    "        if cri_umi != '3':\n",
    "            print(f'ERROR {g}:min-crispr-umi is {cri_umi} but should be 3\\n')\n",
    "\n",
    "    if 'incl_int' in report:\n",
    "        intron = report['incl_int']\n",
    "        if intron != 'true':\n",
    "            print(f'ERROR {g}:include-introns is {intron} but should be true\\n')\n",
    "\n",
    "    if 'create-bam' in report:\n",
    "        bam = report['create-bam']\n",
    "        if chem != 'flex' and bam != 'true':\n",
    "            print(f'ERROR {g}:create-bam is {bam} but should be true\\n')\n",
    "\n",
    "    for a in report['gex_alerts']:\n",
    "        print('GEX ALERTS',g)\n",
    "        print(a,'\\n')\n",
    "    for a in report.get('crispr_alerts',[]):\n",
    "        print('CRI ALERTS',g)\n",
    "        print(a,'\\n')\n",
    "\n",
    "    if sub == 'multi':\n",
    "        if chem == 'flex':\n",
    "            expected = cellranger_expected['flex']['outs'].copy()\n",
    "            per_samp_expected = cellranger_expected['flex']['per_sample'].copy()\n",
    "        else:\n",
    "            expected = cellranger_expected['nonflex']['outs'].copy()\n",
    "            per_samp_expected = cellranger_expected['nonflex']['per_sample'].copy()\n",
    "    elif sub == 'count':\n",
    "        expected = cellranger_expected['count']['outs'].copy()\n",
    "        per_samp_expected = []\n",
    "\n",
    "    #https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/outputs/cr-outputs-ab-overview\n",
    "    #https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/outputs/cr-outputs-crispr-overview\n",
    "    #https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/running-pipelines/cr-cell-annotation-pipeline\n",
    "    if 'CRISPR' in extra or 'Antibody' in extra:\n",
    "        expected.append('multi/count/feature_reference.csv')\n",
    "        per_samp_expected.append('count/feature_reference.csv')\n",
    "        if 'CRISPR' in extra:\n",
    "            per_samp_expected.append('count/crispr_analysis.tar.gz')\n",
    "        if 'Antibody' in extra:\n",
    "            per_samp_expected.append('count/antibody_analysis.tar.gz')\n",
    "    if 'CellAnnotate' in extra:\n",
    "        per_samp_expected.append('count/cell_types.tar.gz')\n",
    "\n",
    "    if sub == 'multi' and report.get('multiplex'):\n",
    "        expected.append('multi/multiplexing_analysis.tar.gz')\n",
    "    \n",
    "    actual = [f.split('/outs/', 1)[1] for f in proc_files if f.split('/')[-1] != 'curated.h5ad']\n",
    "    per_samp_actual = [f for f in actual if f.startswith('per_sample_outs/')]\n",
    "\n",
    "    missing = [f for f in expected if f not in actual]\n",
    "    if missing:\n",
    "        temp_missing = {'group': g}\n",
    "        for m in missing:\n",
    "            temp_missing[m] = 'Y'\n",
    "        proc_missing.append(temp_missing)\n",
    "\n",
    "    if per_samp_expected:\n",
    "        samples = list(set([f.split('/')[8] for f in proc_files if f.split('/')[7] == 'per_sample_outs']))\n",
    "        for s in samples:\n",
    "            expected_samp = [f'per_sample_outs/{s}/{f}' for f in per_samp_expected]\n",
    "            expected.extend(expected_samp)\n",
    "            missing_samp = [f for f in expected_samp if f not in actual]\n",
    "            if missing_samp:\n",
    "                temp_missing = {'group': f'{g}/{s}'}\n",
    "                for m in missing_samp:\n",
    "                    temp_missing[m.replace(f'per_sample_outs/{s}/','')] = 'Y'\n",
    "                proc_missing.append(temp_missing)                \n",
    "\n",
    "    extra = [f for f in actual if f not in expected]\n",
    "    if extra:\n",
    "        print('EXTRA',g)\n",
    "        print(extra,'\\n')\n",
    "\n",
    "    for k,v in report.items():\n",
    "        if k.endswith('_reads'):\n",
    "            assay = k.replace('_reads','')\n",
    "            if g in group_read_counts:\n",
    "                if assay in group_read_counts[g]:\n",
    "                    if v != group_read_counts[g][assay]:\n",
    "                        print('READ COUNT ERROR:',g,assay,\n",
    "                              v,'from proc',\n",
    "                              group_read_counts[g][assay],'from raw',\n",
    "                              v - group_read_counts[g][assay],'diff\\n')\n",
    "                else:\n",
    "                    print('ERROR:assay not found',g,assay,'\\n')\n",
    "            else:\n",
    "                print('ERROR:group not found',g,'\\n')\n",
    "\n",
    "pd.DataFrame(proc_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c238630-5a61-4a7c-b03c-9c46a4bacfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
